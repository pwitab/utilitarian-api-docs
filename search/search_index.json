{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Utilitarian Documentation # Welcome to the Utilitarian documentation. Utilitarian is a multi utility AMR (Automatic Meter Reading) system that gives you and your organisation a simple way to manager AMR operations over multiple energy medias, protocol and metering equipment. Get started # To get a better grip on how Utilitarian and all its component works together check out the Architecture page. If you are interested in what type of communication protocols we support go to the Protocols page. We are continuously adding new meters that we support, and even if we haven't tried out your meters yet we might already support them due to our generic execution environments, or we need to do some work to get them into the system. As of now we are offering Utilitarian as an on-premise services. Our client base of utility companies usually prefer it this way. But keep a look out for the managed cloud version of Utilitarian. To know more about how to run Utilitarian check out the installation page. Get in contact # Simplest way of getting in contact with us is via email: info@utilitarian.io","title":"Home"},{"location":"#utilitarian-documentation","text":"Welcome to the Utilitarian documentation. Utilitarian is a multi utility AMR (Automatic Meter Reading) system that gives you and your organisation a simple way to manager AMR operations over multiple energy medias, protocol and metering equipment.","title":"Utilitarian Documentation"},{"location":"#get-started","text":"To get a better grip on how Utilitarian and all its component works together check out the Architecture page. If you are interested in what type of communication protocols we support go to the Protocols page. We are continuously adding new meters that we support, and even if we haven't tried out your meters yet we might already support them due to our generic execution environments, or we need to do some work to get them into the system. As of now we are offering Utilitarian as an on-premise services. Our client base of utility companies usually prefer it this way. But keep a look out for the managed cloud version of Utilitarian. To know more about how to run Utilitarian check out the installation page.","title":"Get started"},{"location":"#get-in-contact","text":"Simplest way of getting in contact with us is via email: info@utilitarian.io","title":"Get in contact"},{"location":"amr/","text":"Automatic Meter Readings # Pushing architecture # In a pushing architecture the meter pushes data to Utilitarian at fixed intervals. This is a good general approach that simplifies AMR operations. Polling architecture # In a polling architecture Utilitarian is responsible for contacting the meter and reading out the needed values. Many older meters operate using this method. The challenging part is to handle the process of reading out meters when the meter population is very large. Utilitarian allows you to scale up the number of asynchronous worker process over many servers/datacenters so that it can handle the desired load. Mixed architecture # Most advanced metering infrastructures (AMI) have need for both pushing and polling operations. It is very lightweight on the head end system (Utilitarian) to have the meters to have normal operations run in a pushing architecture since we don't have to spend resources on polling the meters at fixed interval. But there is always need for the ability to poll. For example we might be missing meter readings from an interval due to communication outage and need to get the values for invoicing. Then we most certainly need to poll the meter for the values. We might also want to read values that are not part of the normal push operations at certain intervals. For example: statistics about radio reception level. Other times we might need to write values to the meter, for example correcting time. How Utilitarian handles AMR operations. # Every meter reading that is collected is turned into a NewMeterReading message and put on the message queue. The Utilitarian API then receives the messages and saves it as meter readings in the database. Pushing meters sends their meters readings to meter specific or protocol specific receivers that convert the data from the meter into NewMeterReading messages. Polled meters are controlled via AmrTask . AmrTasks can be grouped in an AmrTaskGroup . AmrTasks # An AmrTask controls how a polling of a meter is done. name A unique name for the task description Longer description of the task schedule Decides on what schedule the task should be run. Time is in UTC! quarterly , Task is run every 15 min, at 00, 15, 30 and 45. hourly , Task is run every hour at 00 daily , Task is run every day at 00:00 weekly , Task is run every monday at 00:00 monthly , Task is run the 1st of every month at 00:00 yearly , Task is run the 1st of January at 00:00 every year run_offet_seconds If you are in another timezone than UTC you might want to run the task at different times than what is specified in the schedule. Many meters also have a slight clock synchronisation error so a value for a 00:00 might not have been registered in the meter when Utilitarian want to poll it. Setting the run_offset_seconds will enable you to delay the execution of the task for that amount of seconds. You can only set as many seconds as there is until the next run of the task. For example you can only delay a quarterly task maximum of 15 minutes. Note It is not guarenteed that your task will be executed at the specific time set. But it will not be executed before that time. It all depends on the workload of the workers. meter_execution_type Decides in what execution context the task should run. Utilitarian has a number of implemented meters exeution contexts that can be used. Example: generic_lis200 task_call In the specified meter execution context there will be a number of available calls to make to the meter. Example: set_time , read_archive_by_offset_seconds task_kwargs For every task_call there might be some additional info needed to run the task. These are supplied as key value arguments in the task_kwargs field. Example for task_call= read_archive_by_offset_seconds -> {\"offset_seconds\": 1000, \"archive_number\": 3, \"control_position\": 3} AmrTaskGroups # Several AmrTasks can be grouped together in an AmrTaskGroup. This is useful for managing all the different tasks for a specific meter type. Connecting to Meter Devices # Meter Devices can have several AmrTaskGroup connected to it via the task_groups field. This should be the normal operation of managing tasks for a meter. But sometimes you might want to do some extra tasks on specific meters and then it is possible to add tasks via the meter_tasks field. Tasks defined there will only run for that meter. Direct invocation # It is possible to invoke a task directly via the API. No AmrTask is saved in the database but the same arguments are used. There tasks will be sent to the amr.on_demand queue and it is recommended to run a separate worker for this queue if you want the results fast. Example: { \"meter_device\": \"890b61bb-6594-48db-ad90-584d56754455\", \"task_call\": \"read_archive_by_offset_seconds\", \"task_kwargs\": { \"offset_seconds\": 1000, \"archive_number\": 3, \"control_position\": 3 }, \"run_offset_seconds\": 0, \"meter_execution_type\": \"generic_lis200\" }","title":"AMR in Utilitarian"},{"location":"amr/#automatic-meter-readings","text":"","title":"Automatic Meter Readings"},{"location":"amr/#pushing-architecture","text":"In a pushing architecture the meter pushes data to Utilitarian at fixed intervals. This is a good general approach that simplifies AMR operations.","title":"Pushing architecture"},{"location":"amr/#polling-architecture","text":"In a polling architecture Utilitarian is responsible for contacting the meter and reading out the needed values. Many older meters operate using this method. The challenging part is to handle the process of reading out meters when the meter population is very large. Utilitarian allows you to scale up the number of asynchronous worker process over many servers/datacenters so that it can handle the desired load.","title":"Polling architecture"},{"location":"amr/#mixed-architecture","text":"Most advanced metering infrastructures (AMI) have need for both pushing and polling operations. It is very lightweight on the head end system (Utilitarian) to have the meters to have normal operations run in a pushing architecture since we don't have to spend resources on polling the meters at fixed interval. But there is always need for the ability to poll. For example we might be missing meter readings from an interval due to communication outage and need to get the values for invoicing. Then we most certainly need to poll the meter for the values. We might also want to read values that are not part of the normal push operations at certain intervals. For example: statistics about radio reception level. Other times we might need to write values to the meter, for example correcting time.","title":"Mixed architecture"},{"location":"amr/#how-utilitarian-handles-amr-operations","text":"Every meter reading that is collected is turned into a NewMeterReading message and put on the message queue. The Utilitarian API then receives the messages and saves it as meter readings in the database. Pushing meters sends their meters readings to meter specific or protocol specific receivers that convert the data from the meter into NewMeterReading messages. Polled meters are controlled via AmrTask . AmrTasks can be grouped in an AmrTaskGroup .","title":"How Utilitarian handles AMR operations."},{"location":"amr/#amrtasks","text":"An AmrTask controls how a polling of a meter is done. name A unique name for the task description Longer description of the task schedule Decides on what schedule the task should be run. Time is in UTC! quarterly , Task is run every 15 min, at 00, 15, 30 and 45. hourly , Task is run every hour at 00 daily , Task is run every day at 00:00 weekly , Task is run every monday at 00:00 monthly , Task is run the 1st of every month at 00:00 yearly , Task is run the 1st of January at 00:00 every year run_offet_seconds If you are in another timezone than UTC you might want to run the task at different times than what is specified in the schedule. Many meters also have a slight clock synchronisation error so a value for a 00:00 might not have been registered in the meter when Utilitarian want to poll it. Setting the run_offset_seconds will enable you to delay the execution of the task for that amount of seconds. You can only set as many seconds as there is until the next run of the task. For example you can only delay a quarterly task maximum of 15 minutes. Note It is not guarenteed that your task will be executed at the specific time set. But it will not be executed before that time. It all depends on the workload of the workers. meter_execution_type Decides in what execution context the task should run. Utilitarian has a number of implemented meters exeution contexts that can be used. Example: generic_lis200 task_call In the specified meter execution context there will be a number of available calls to make to the meter. Example: set_time , read_archive_by_offset_seconds task_kwargs For every task_call there might be some additional info needed to run the task. These are supplied as key value arguments in the task_kwargs field. Example for task_call= read_archive_by_offset_seconds -> {\"offset_seconds\": 1000, \"archive_number\": 3, \"control_position\": 3}","title":"AmrTasks"},{"location":"amr/#amrtaskgroups","text":"Several AmrTasks can be grouped together in an AmrTaskGroup. This is useful for managing all the different tasks for a specific meter type.","title":"AmrTaskGroups"},{"location":"amr/#connecting-to-meter-devices","text":"Meter Devices can have several AmrTaskGroup connected to it via the task_groups field. This should be the normal operation of managing tasks for a meter. But sometimes you might want to do some extra tasks on specific meters and then it is possible to add tasks via the meter_tasks field. Tasks defined there will only run for that meter.","title":"Connecting to Meter Devices"},{"location":"amr/#direct-invocation","text":"It is possible to invoke a task directly via the API. No AmrTask is saved in the database but the same arguments are used. There tasks will be sent to the amr.on_demand queue and it is recommended to run a separate worker for this queue if you want the results fast. Example: { \"meter_device\": \"890b61bb-6594-48db-ad90-584d56754455\", \"task_call\": \"read_archive_by_offset_seconds\", \"task_kwargs\": { \"offset_seconds\": 1000, \"archive_number\": 3, \"control_position\": 3 }, \"run_offset_seconds\": 0, \"meter_execution_type\": \"generic_lis200\" }","title":"Direct invocation"},{"location":"amr_um/","text":"Unified Messaging for Automatic Meter Readings # AMR UM is a project trying to define clearer messages that are focused on AMR operations. In metering it is common to use CIM (IEC 61968) to model your business processes and define messages for different type of operations. But it is all on quite high level. It is also a bit ambiguous how to use the models. There are several possibilities for the DSO (Distribution System Operators) to model the same data so focusing to comply with CIM in messages within Utilitarian would not be beneficial. Instead we are focusing on making clearly defined messages and document the usage well. We will if needed, provide translation services for CIM or other formats. But as of now it is up to the user to hook in to the message stream and get the data they need. We have developed a helper library in Python to build objects and make payloads for AMR UM messages: Check it out on GitHub Message Types # DLMS Push Message # Contains a push message (DataNotification) from a DLMS Meter { \"payload\" : \"base64encodedbytes\" , \"transport\" : \"udp\" , \"source_address\" : 'ip_address_of_origin' , \"source_port\" : 5678 , \"application_context\" : \"dsmr\" , \"dlms_wrapper\" : { \"source_wport\" : 1 , \"destiniation_wport\" : 10 , \"version\" : 1 } } New Meter Reading # Message containing a single meter reading { \"meter\" : \"FIOR78464920374\" , \"series\" : \"7-0-13-26-0-101\" , \"timestamp\" : \"2019-02-24T06:00:00T+01:00\" , \"value\" : \"14567.000\" }","title":"Messaging"},{"location":"amr_um/#unified-messaging-for-automatic-meter-readings","text":"AMR UM is a project trying to define clearer messages that are focused on AMR operations. In metering it is common to use CIM (IEC 61968) to model your business processes and define messages for different type of operations. But it is all on quite high level. It is also a bit ambiguous how to use the models. There are several possibilities for the DSO (Distribution System Operators) to model the same data so focusing to comply with CIM in messages within Utilitarian would not be beneficial. Instead we are focusing on making clearly defined messages and document the usage well. We will if needed, provide translation services for CIM or other formats. But as of now it is up to the user to hook in to the message stream and get the data they need. We have developed a helper library in Python to build objects and make payloads for AMR UM messages: Check it out on GitHub","title":"Unified Messaging for Automatic Meter Readings"},{"location":"amr_um/#message-types","text":"","title":"Message Types"},{"location":"amr_um/#dlms-push-message","text":"Contains a push message (DataNotification) from a DLMS Meter { \"payload\" : \"base64encodedbytes\" , \"transport\" : \"udp\" , \"source_address\" : 'ip_address_of_origin' , \"source_port\" : 5678 , \"application_context\" : \"dsmr\" , \"dlms_wrapper\" : { \"source_wport\" : 1 , \"destiniation_wport\" : 10 , \"version\" : 1 } }","title":"DLMS Push Message"},{"location":"amr_um/#new-meter-reading","text":"Message containing a single meter reading { \"meter\" : \"FIOR78464920374\" , \"series\" : \"7-0-13-26-0-101\" , \"timestamp\" : \"2019-02-24T06:00:00T+01:00\" , \"value\" : \"14567.000\" }","title":"New Meter Reading"},{"location":"architecture/","text":"Architecture # Utilitarian is a multi utility AMR system built for the Cloud. Multi utility # Utilitarian is built to manage any energy type, meter and communications protocol. Scalable # Utilitarian is a decentralized system that can be deployed across as many servers as needed to fit your use case. By using a message broker workloads are split across asynchronous worker processes for time consuming like polling for meter data. Workers can be scaled independently for fit your load. Receiving of push meter data is made in separate services that can all be individually scaled for the needed load. Multi tenant # Utilitarian is built with multi tenancy using a same schema approach. We organize meters and meter information around the concept of Organizations. A User can be part of several Organizations. Reliable # Several instances can be run of every service and they can be run in different environments, for example if you want to split load between different data centers or availability zones Utilitarian does not limit you. Services can be load balanced using standard technologies. The main points of failure are the Postgres database and the RabbitMQ message broker, but these risks can be mitigated using High Availability (HA) setups and standard database reliability techniques. Secure # Depending on internal and regulatory requirements you can run Utilitarian with a range of security levels and settings so that you can find the best fit for your organization. Configurable # You can just run the services needed for your specific use case. Open # Utilitarian is designed to enable users to easily build their own integrations and subsystems if needed. It is possible to hook into the message streams from any application using the message payload documentation. Utilitarian is built using open source technologies and libraries. We have also open sourced many of our own libraries that we use within Utilitarian. Cloud Native # Utilitarian is built for the cloud. We deliver all services via docker containers so you can decide how and where to run the application. You can run it on a single machine or deploy using Kubernetes in a hybrid cloud with multiple data centers.","title":"Overview"},{"location":"architecture/#architecture","text":"Utilitarian is a multi utility AMR system built for the Cloud.","title":"Architecture"},{"location":"architecture/#multi-utility","text":"Utilitarian is built to manage any energy type, meter and communications protocol.","title":"Multi utility"},{"location":"architecture/#scalable","text":"Utilitarian is a decentralized system that can be deployed across as many servers as needed to fit your use case. By using a message broker workloads are split across asynchronous worker processes for time consuming like polling for meter data. Workers can be scaled independently for fit your load. Receiving of push meter data is made in separate services that can all be individually scaled for the needed load.","title":"Scalable"},{"location":"architecture/#multi-tenant","text":"Utilitarian is built with multi tenancy using a same schema approach. We organize meters and meter information around the concept of Organizations. A User can be part of several Organizations.","title":"Multi tenant"},{"location":"architecture/#reliable","text":"Several instances can be run of every service and they can be run in different environments, for example if you want to split load between different data centers or availability zones Utilitarian does not limit you. Services can be load balanced using standard technologies. The main points of failure are the Postgres database and the RabbitMQ message broker, but these risks can be mitigated using High Availability (HA) setups and standard database reliability techniques.","title":"Reliable"},{"location":"architecture/#secure","text":"Depending on internal and regulatory requirements you can run Utilitarian with a range of security levels and settings so that you can find the best fit for your organization.","title":"Secure"},{"location":"architecture/#configurable","text":"You can just run the services needed for your specific use case.","title":"Configurable"},{"location":"architecture/#open","text":"Utilitarian is designed to enable users to easily build their own integrations and subsystems if needed. It is possible to hook into the message streams from any application using the message payload documentation. Utilitarian is built using open source technologies and libraries. We have also open sourced many of our own libraries that we use within Utilitarian.","title":"Open"},{"location":"architecture/#cloud-native","text":"Utilitarian is built for the cloud. We deliver all services via docker containers so you can decide how and where to run the application. You can run it on a single machine or deploy using Kubernetes in a hybrid cloud with multiple data centers.","title":"Cloud Native"},{"location":"async_workers/","text":"Async Workers # Most of the heavy lifting in Utilitarian is made in asynchronous worker processes outside of the request-response cycle of the Utilitarian API. Tasks are transported to workers using RabbitMQ. Note We are using Celery to manage tasks and workers. You can learn a lot in their documentation on how to run and monitor tasks and workers. Queues # We separate different types of tasks on different queues. This makes you able to spin up more workers dedicated to certain queues depending on your load. It also enables you to lower your infrastructure bill since some queues used for AMR Tasks are only used daily or monthly depending on your AMR requirements. For example: you could just spin up the monthly amr task workers for the first day of the month to handle all the work. When all values have been collected you can shut it donw until next month. If you start a worker without dedicated queues it will consume from all queues. List of queues. # utilitarian.tasks.default utilitarian.tasks.amr utilitarian.tasks.amr.scheduled.quarterly utilitarian.tasks.amr.scheduled.hourly utilitarian.tasks.amr.scheduled.daily utilitarian.tasks.amr.scheduled.monthly utilitarian.tasks.amr.scheduled.yearly utilitarian.tasks.amr.on_demand utilitarian.tasks.commits utilitarian.tasks.commits.meter_readings utilitarian.tasks.commits.meter_system_events utilitarian.tasks.external utilitarian.tasks.external.publish utilitarian.tasks.cleanup Workers # A worker is a process that consumes tasks and execute them. Since the tasks are delivered via RabbitMQ you can split the processes up over several servers if you need to. It is possible to run a worker with increased concurrency. That means that the process will spawn several subprocess' to process tasks concurrently. The concurrency settings are something that might need tuning since you want to maximize your server utilization of CPU and memory and depending on the type of tasks (the queues) the workers consumes the load on the server might differ. It all depends on what kind of meters Utilitarian is reading and how many meters and at what frequency the readings are made. Since this can vary very much from customer to customer we cannot give you exact numbers on how to set up you workers but we will help you optimize for you unique circumstances. Run a worker # We use the same docker image as the Utilitarian API, with all the same settings. If you are using docker-compose you define utilitarian-worker : image : quay.io/pwit/utilitarian:version hostname : utilitarian-worker command : celery -A utilitarian worker --loglevel=INFO environment : *common_environment # unpacks settings so you only have to define it once in the docker-compose Optimizing workers # Depending on how you run Utilitarian, ie. what type of meter you use, how many etc, the load can look very different. That is why we split the work on separate queues so that it is possible to start more workers that just handle the high load queues Start worker for specific queues # Your can specify which queues the worker should consumer from at start-up by using the -Q option and give a comma separated list with queue names. celery -A utilitarian worker -l info -Q utilitarian.tasks.amr.scheduled.quarterly,utilitarian.tasks.amr.on_demand Worker concurrency # Workers start up child processes to handle tasks concurrently. This defaults to the number of CPUs available on the system. But since most AMR task are I/O bound we could start up even more to process more tasks concurrently. Exactly how many workers to run and how high the concurrency should be is something that is highly dependant on exacly how you use Utilitarian and you will have to experiment to find the best utilization of your resources. It is generally seen to be better to start more workers than keep on increasing concurrency. You start a worker with specific concurrency using the --concurrency option celery -A utilitarian worker -l info --concurrency=10 How high concurrency and the amount of workers is something you will have to weigh against yor need and budget (more workers might need more servers). It might be fine for you that daily polling tasks take a couple of hours to poll all meter. But if not start up more workers and increase concurrency higher concurrency. Just keep track of your memory and CPU utilization. Beat # The beat process is the scheduler for scheduled or periodic tasks. Utilitarian has a number of maintenance tasks that needs to be run at certain intervals. The beat process is keeping control of when a task was run last and initiates new tasks when they are scheduled. It is also the beat process that initiates scheduled polling of meter data from polled meters. Only run one single beat process. If not you will end up with duplicate tasks. Duplicate tasks are usually ok, but will increase load on the system and might lead to higher operations costs such as data transfer cost. Run the beat process # utilitarian-beat : image : quay.io/pwit/utilitarian:version hostname : utilitarian-beat command : celery -A utilitarian beat --loglevel=INFO environment : *common_environment # unpacks settings so you only have to define it once in the docker-compose","title":"Async Workers"},{"location":"async_workers/#async-workers","text":"Most of the heavy lifting in Utilitarian is made in asynchronous worker processes outside of the request-response cycle of the Utilitarian API. Tasks are transported to workers using RabbitMQ. Note We are using Celery to manage tasks and workers. You can learn a lot in their documentation on how to run and monitor tasks and workers.","title":"Async Workers"},{"location":"async_workers/#queues","text":"We separate different types of tasks on different queues. This makes you able to spin up more workers dedicated to certain queues depending on your load. It also enables you to lower your infrastructure bill since some queues used for AMR Tasks are only used daily or monthly depending on your AMR requirements. For example: you could just spin up the monthly amr task workers for the first day of the month to handle all the work. When all values have been collected you can shut it donw until next month. If you start a worker without dedicated queues it will consume from all queues.","title":"Queues"},{"location":"async_workers/#list-of-queues","text":"utilitarian.tasks.default utilitarian.tasks.amr utilitarian.tasks.amr.scheduled.quarterly utilitarian.tasks.amr.scheduled.hourly utilitarian.tasks.amr.scheduled.daily utilitarian.tasks.amr.scheduled.monthly utilitarian.tasks.amr.scheduled.yearly utilitarian.tasks.amr.on_demand utilitarian.tasks.commits utilitarian.tasks.commits.meter_readings utilitarian.tasks.commits.meter_system_events utilitarian.tasks.external utilitarian.tasks.external.publish utilitarian.tasks.cleanup","title":"List of queues."},{"location":"async_workers/#workers","text":"A worker is a process that consumes tasks and execute them. Since the tasks are delivered via RabbitMQ you can split the processes up over several servers if you need to. It is possible to run a worker with increased concurrency. That means that the process will spawn several subprocess' to process tasks concurrently. The concurrency settings are something that might need tuning since you want to maximize your server utilization of CPU and memory and depending on the type of tasks (the queues) the workers consumes the load on the server might differ. It all depends on what kind of meters Utilitarian is reading and how many meters and at what frequency the readings are made. Since this can vary very much from customer to customer we cannot give you exact numbers on how to set up you workers but we will help you optimize for you unique circumstances.","title":"Workers"},{"location":"async_workers/#run-a-worker","text":"We use the same docker image as the Utilitarian API, with all the same settings. If you are using docker-compose you define utilitarian-worker : image : quay.io/pwit/utilitarian:version hostname : utilitarian-worker command : celery -A utilitarian worker --loglevel=INFO environment : *common_environment # unpacks settings so you only have to define it once in the docker-compose","title":"Run a worker"},{"location":"async_workers/#optimizing-workers","text":"Depending on how you run Utilitarian, ie. what type of meter you use, how many etc, the load can look very different. That is why we split the work on separate queues so that it is possible to start more workers that just handle the high load queues","title":"Optimizing workers"},{"location":"async_workers/#start-worker-for-specific-queues","text":"Your can specify which queues the worker should consumer from at start-up by using the -Q option and give a comma separated list with queue names. celery -A utilitarian worker -l info -Q utilitarian.tasks.amr.scheduled.quarterly,utilitarian.tasks.amr.on_demand","title":"Start worker for specific queues"},{"location":"async_workers/#worker-concurrency","text":"Workers start up child processes to handle tasks concurrently. This defaults to the number of CPUs available on the system. But since most AMR task are I/O bound we could start up even more to process more tasks concurrently. Exactly how many workers to run and how high the concurrency should be is something that is highly dependant on exacly how you use Utilitarian and you will have to experiment to find the best utilization of your resources. It is generally seen to be better to start more workers than keep on increasing concurrency. You start a worker with specific concurrency using the --concurrency option celery -A utilitarian worker -l info --concurrency=10 How high concurrency and the amount of workers is something you will have to weigh against yor need and budget (more workers might need more servers). It might be fine for you that daily polling tasks take a couple of hours to poll all meter. But if not start up more workers and increase concurrency higher concurrency. Just keep track of your memory and CPU utilization.","title":"Worker concurrency"},{"location":"async_workers/#beat","text":"The beat process is the scheduler for scheduled or periodic tasks. Utilitarian has a number of maintenance tasks that needs to be run at certain intervals. The beat process is keeping control of when a task was run last and initiates new tasks when they are scheduled. It is also the beat process that initiates scheduled polling of meter data from polled meters. Only run one single beat process. If not you will end up with duplicate tasks. Duplicate tasks are usually ok, but will increase load on the system and might lead to higher operations costs such as data transfer cost.","title":"Beat"},{"location":"async_workers/#run-the-beat-process","text":"utilitarian-beat : image : quay.io/pwit/utilitarian:version hostname : utilitarian-beat command : celery -A utilitarian beat --loglevel=INFO environment : *common_environment # unpacks settings so you only have to define it once in the docker-compose","title":"Run the beat process"},{"location":"component_settings/","text":"Component Settings # Our docker images are handed settings via Environment Variables Utilitarian API # UTILITARIAN_DEBUG # Enables debugging features. Defaults to False . Warning Do not use in production! UTILITARIAN_LOGLEVEL # Sets the loglevel of the application. Valid inputs are: debug , info , warning , error , critical . Defaults to info DATABASE_URL # Connection string to Postgres database. SECRET_KEY # Used for internal cryptographic signing and cryptographic methods. Should be generated once at first deploy time. Recommended length is 50 characters. Warning The SECRET_KEY plays an important role in securing the application. Make sure it remains secret. ALLOWED_HOSTS # A list of allowed host for the application. The application will only accept requests with a host in this list. Example: example.com, 127.0.0.1 . ACCOUNT_ALLOW_REGISTRATION # If you want to allow user registration in the application. Defaults to False . AMQP_CONNECTION_STRING # AMQP Connection string to RabbitMQ broker. Example: amqp://guest:guest@rabbitmq:5672// . AMQP_PUBLISH_TO # RabbitMQ exchange where messages from Utilitarian API is published. SECURE_SSL_REDIRECT # It is better to use DNS or a reverse proxy for this functionality but if that is not possible setting to True will redirect all non HTTPS requests to HTTPS. Defaults to False . SECURE_HSTS_SECONDS # If set to a non-zero integer value the application will set the HTTP Strict Transport Security header on all responses that do not already have it. This will tell browsers and clients that the application is only to be served under HTTPS. When enabling you should always set to a low value (60) and increase it after it works. Defaults to 0 . Warning Using this setting wrongly can make your application unaccessable for quite some time. Please see the full documentation of this feature before doing any changes. SECURE_CONTENT_TYPE_NOSNIFF # If True, Utilitarian API sets the X-Content-Type-Options: nosniff header on all responses that do not already have it. Defaults to True . SECURE_BROWSER_XSS_FILTER # If True, Utilitarian API sets the X-XSS-Protection: 1; mode=block header on all responses that do not already have it. Defaults to True . SESSION_COOKIE_SECURE # Whether to use a secure cookie for the session cookie. If this is set to True, the cookie will be marked as \u201csecure,\u201d which means browsers may ensure that the cookie is only sent under an HTTPS connection. Defaults to False . CSRF_COOKIE_SECURE # Whether to use a secure cookie for the CSRF cookie. If this is set to True, the cookie will be marked as \u201csecure,\u201d which means browsers may ensure that the cookie is only sent under an HTTPS connection. Defaults to False . USE_X_FORWARDED_HOST # A boolean that specifies whether to use the X-Forwarded-Host header in preference to the Host header. This should only be enabled if a proxy which sets this header is in use. This setting takes priority over USE_X_FORWARDED_PORT. Per RFC 7239#page-7 , the X-Forwarded-Host header can include the port number, in which case you shouldn\u2019t use USE_X_FORWARDED_PORT. Defaults to False . USE_X_FORWARDED_PORT # A boolean that specifies whether to use the X-Forwarded-Port header in preference to the SERVER_PORT META variable. This should only be enabled if a proxy which sets this header is in use. USE_X_FORWARDED_HOST takes priority over this setting. USE_X_FORWARDED_PROTO # X-Forwarded-Proto header that comes from our proxy, and any time its value is 'https', then the request is guaranteed to be secure (i.e., it originally came in via HTTPS). You should only set this setting if you control your proxy or have some other guarantee that it sets/strips this header appropriately Defaults to False Warning Modifying this setting can compromise your Utilitarian API\u2019s security. Ensure you fully understand your setup before changing it. Make sure ALL of the following are true before setting this (assuming the values from the example above): Your Utilitarian API is behind a proxy. Your proxy strips the X-Forwarded-Proto header from all incoming requests. In other words, if end users include that header in their requests, the proxy will discard it. Your proxy sets the X-Forwarded-Proto header and sends it to Utilitarian API, but only for requests that originally come in via HTTPS. If any of those are not true, you should keep this setting set to False. HEALTHCHECK_DISABLED # Disables the health check endpoint /_health/ Defaults to False DATA_RETENTION_DAYS # Sets the amount of days you want to keep data in Utilitarian. Only affects time series data as meter readings and meter system events. For example: 365 days in a year. If you want to keep all data that is younger than 2 years and discard everything that is older than 2 years you would set DATA_RETENTION_DAYS=730 . If the environment variable is not set data is kept forever. Defaults to None STAGED_DATA_RETENTION_DAYS # Sets the amount of days you want to keep staged data in Utilitarian. For example staged meter readings. It is assumed that staged data is short lived so you shouldn't have to long data retention for staged data. Defaults to 30 days CORS_WHITELIST # A list of origins that are authorized to make cross-site HTTP requests. An Origin is defined by the CORS RFC Section 3.2 as a URI scheme + hostname + port, or the special value 'null'. Default ports (HTTPS = 443, HTTP = 80) are optional here. Defaults to []. Example: https://example.com,https://sub.example.com,http://localhost:8080,http://127.0.0.1:9000 CORS_ALLOW_ALL # If True, the whitelist will not be used and all origins will be accepted. Defaults to False. Useful for testing but should not be used in production. JWT_SIGNING_KEY # Holds the value of the JWT_SINGING_KEY that is used to sign JSON Web Tokens. Defaults to value of SECRET_KEY . It is useful to have a separate key if you want the possibility to invalidate all current tokens. It is recomended to set up a key rotation schedule. JWT_EXPIRATION_TIME_SECONDS # How long a JWT should be valid in seconds. Defaults to 1200 seconds (20 min) ALLOW_TOKEN_AUTH # If True, token authentication will be allowed on the API. Defaults to True. It is recomended if you expose the API to \"normal\" users via JWT Auth to disallow Token Auth so that no endpoints are not exposed. Instead you run septarate instances for your internal applications that allows TokenAuth. DEFAULT_METER_TIMEZONE # Sets the default meter timezone that is used for MeterDeviceConfig. Deaults to \"UTC\". For possible values see https://en.wikipedia.org/wiki/List_of_tz_database_time_zones All values might not be available on your system. DLMS UDP Server # DLMS_UDP_SERVER_DEBUG # Will set the loglevel to debug. Defaults to False AMQP_CONNECTION_STRING # AMQP Connection string to RabbitMQ broker. Example: amqp://guest:guest@rabbitmq:5672// . AMQP_EXCHANGE_NAME # The exchange where messages will be published. Defaults to utilitarian AMQP_DEFAULT_QUEUE_NAME # To ensure that the broker always have a queue that will receive our messages we declare in both publishers and subscribers. This is the name of the queue. Defaults to utilitarian.dlms_push_messages AMQP_DEFAULT_QUEUE_ROUTING_KEY # The routing key we want to bind to the default key. Defaults to new_dlms_push_message.#\"` UTILITARIAN_APPLICATION_CONTEXT # Since there are a few different variations (companion standards) to DLMS/COSEM and some implements some things differntly we add the application context of receiving server to add it to any outgoing messages. You should run a separate instance of the UDP server for each different application context. It is not possible to see in the messages what companion standard the meter is using so we separate them by setting them up against different server. Defaults to units11291 DLMS PROCESSOR # DLMS_CONSUMER_DEBUG # Will set the loglevel to debug. Defaults to False AMQP_CONNECTION_STRING # AMQP Connection string to RabbitMQ broker. Example: amqp://guest:guest@rabbitmq:5672// . DLMS_CONSUMER_CONSUME_FROM # The queue from where to consume messages. Defaults to utilitarian.dlms_push_messages DLMS_CONSUMER_PREFETCH_COUNT # The consumer prefetch count, how many unacknowledged messages the consumer is allowed to have in memory. Default to 100 DLMS_CONSUMER_PUBLISH_TO # Exchange to publish meter readings to. Defaults to utilitarian UTILITARIAN_BASE_URL # The base url for Utilitarian API. Example: https://utilitarian.example.com:8000 UTILITARIAN_AUTH_TOKEN # Token for authentication to Utilitarian API. Example: 9c72cba03f4920dcb4b62c4d2723fe5718990024 UTILITARIAN_REQUEST_TIMEOUT # Request timeout for HTTP calls to Utilitarian API. IF you start seeing a high rate of TimeoutErrors your database might not have enough resources or you need to load balance the API. But increasing the timeout could solve you problems short-term. Defined in seconds Defaults to 15 Utilitarian Poster # POSTER_DEBUG # Enables debug. Sets loglevel to debug Defaults to False AMQP_CONNECTION_STRING # AMQP Connection string to RabbitMQ broker. Example: amqp://guest:guest@rabbitmq:5672// . POSTER_CONSUME_FROM # Name of the queues to consume from. Defaults to utilitarian.new_meter_readings POSTER_PREFETCH_COUNT # The consumer prefetch count, how many unacknowledged messages the consumer is allowed to have in memory. Default to 100 UTILITARIAN_BASE_URL # The base url for Utilitarian API. Example: https://utilitarian.example.com:8000 UTILITARIAN_AUTH_TOKEN # Token for authentication to Utilitarian API. Example: 9c72cba03f4920dcb4b62c4d2723fe5718990024 UTILITARIAN_REQUEST_TIMEOUT # Request timeout for HTTP calls to Utilitarian API. IF you start seeing a high rate of TimeoutErrors your database might not have enough resources or you need to load balance the API. But increasing the timeout could solve you problems short-term. Defined in seconds Defaults to 15","title":"Component Configuration"},{"location":"component_settings/#component-settings","text":"Our docker images are handed settings via Environment Variables","title":"Component Settings"},{"location":"component_settings/#utilitarian-api","text":"","title":"Utilitarian API"},{"location":"component_settings/#utilitarian_debug","text":"Enables debugging features. Defaults to False . Warning Do not use in production!","title":"UTILITARIAN_DEBUG"},{"location":"component_settings/#utilitarian_loglevel","text":"Sets the loglevel of the application. Valid inputs are: debug , info , warning , error , critical . Defaults to info","title":"UTILITARIAN_LOGLEVEL"},{"location":"component_settings/#database_url","text":"Connection string to Postgres database.","title":"DATABASE_URL"},{"location":"component_settings/#secret_key","text":"Used for internal cryptographic signing and cryptographic methods. Should be generated once at first deploy time. Recommended length is 50 characters. Warning The SECRET_KEY plays an important role in securing the application. Make sure it remains secret.","title":"SECRET_KEY"},{"location":"component_settings/#allowed_hosts","text":"A list of allowed host for the application. The application will only accept requests with a host in this list. Example: example.com, 127.0.0.1 .","title":"ALLOWED_HOSTS"},{"location":"component_settings/#account_allow_registration","text":"If you want to allow user registration in the application. Defaults to False .","title":"ACCOUNT_ALLOW_REGISTRATION"},{"location":"component_settings/#amqp_connection_string","text":"AMQP Connection string to RabbitMQ broker. Example: amqp://guest:guest@rabbitmq:5672// .","title":"AMQP_CONNECTION_STRING"},{"location":"component_settings/#amqp_publish_to","text":"RabbitMQ exchange where messages from Utilitarian API is published.","title":"AMQP_PUBLISH_TO"},{"location":"component_settings/#secure_ssl_redirect","text":"It is better to use DNS or a reverse proxy for this functionality but if that is not possible setting to True will redirect all non HTTPS requests to HTTPS. Defaults to False .","title":"SECURE_SSL_REDIRECT"},{"location":"component_settings/#secure_hsts_seconds","text":"If set to a non-zero integer value the application will set the HTTP Strict Transport Security header on all responses that do not already have it. This will tell browsers and clients that the application is only to be served under HTTPS. When enabling you should always set to a low value (60) and increase it after it works. Defaults to 0 . Warning Using this setting wrongly can make your application unaccessable for quite some time. Please see the full documentation of this feature before doing any changes.","title":"SECURE_HSTS_SECONDS"},{"location":"component_settings/#secure_content_type_nosniff","text":"If True, Utilitarian API sets the X-Content-Type-Options: nosniff header on all responses that do not already have it. Defaults to True .","title":"SECURE_CONTENT_TYPE_NOSNIFF"},{"location":"component_settings/#secure_browser_xss_filter","text":"If True, Utilitarian API sets the X-XSS-Protection: 1; mode=block header on all responses that do not already have it. Defaults to True .","title":"SECURE_BROWSER_XSS_FILTER"},{"location":"component_settings/#session_cookie_secure","text":"Whether to use a secure cookie for the session cookie. If this is set to True, the cookie will be marked as \u201csecure,\u201d which means browsers may ensure that the cookie is only sent under an HTTPS connection. Defaults to False .","title":"SESSION_COOKIE_SECURE"},{"location":"component_settings/#csrf_cookie_secure","text":"Whether to use a secure cookie for the CSRF cookie. If this is set to True, the cookie will be marked as \u201csecure,\u201d which means browsers may ensure that the cookie is only sent under an HTTPS connection. Defaults to False .","title":"CSRF_COOKIE_SECURE"},{"location":"component_settings/#use_x_forwarded_host","text":"A boolean that specifies whether to use the X-Forwarded-Host header in preference to the Host header. This should only be enabled if a proxy which sets this header is in use. This setting takes priority over USE_X_FORWARDED_PORT. Per RFC 7239#page-7 , the X-Forwarded-Host header can include the port number, in which case you shouldn\u2019t use USE_X_FORWARDED_PORT. Defaults to False .","title":"USE_X_FORWARDED_HOST"},{"location":"component_settings/#use_x_forwarded_port","text":"A boolean that specifies whether to use the X-Forwarded-Port header in preference to the SERVER_PORT META variable. This should only be enabled if a proxy which sets this header is in use. USE_X_FORWARDED_HOST takes priority over this setting.","title":"USE_X_FORWARDED_PORT"},{"location":"component_settings/#use_x_forwarded_proto","text":"X-Forwarded-Proto header that comes from our proxy, and any time its value is 'https', then the request is guaranteed to be secure (i.e., it originally came in via HTTPS). You should only set this setting if you control your proxy or have some other guarantee that it sets/strips this header appropriately Defaults to False Warning Modifying this setting can compromise your Utilitarian API\u2019s security. Ensure you fully understand your setup before changing it. Make sure ALL of the following are true before setting this (assuming the values from the example above): Your Utilitarian API is behind a proxy. Your proxy strips the X-Forwarded-Proto header from all incoming requests. In other words, if end users include that header in their requests, the proxy will discard it. Your proxy sets the X-Forwarded-Proto header and sends it to Utilitarian API, but only for requests that originally come in via HTTPS. If any of those are not true, you should keep this setting set to False.","title":"USE_X_FORWARDED_PROTO"},{"location":"component_settings/#healthcheck_disabled","text":"Disables the health check endpoint /_health/ Defaults to False","title":"HEALTHCHECK_DISABLED"},{"location":"component_settings/#data_retention_days","text":"Sets the amount of days you want to keep data in Utilitarian. Only affects time series data as meter readings and meter system events. For example: 365 days in a year. If you want to keep all data that is younger than 2 years and discard everything that is older than 2 years you would set DATA_RETENTION_DAYS=730 . If the environment variable is not set data is kept forever. Defaults to None","title":"DATA_RETENTION_DAYS"},{"location":"component_settings/#staged_data_retention_days","text":"Sets the amount of days you want to keep staged data in Utilitarian. For example staged meter readings. It is assumed that staged data is short lived so you shouldn't have to long data retention for staged data. Defaults to 30 days","title":"STAGED_DATA_RETENTION_DAYS"},{"location":"component_settings/#cors_whitelist","text":"A list of origins that are authorized to make cross-site HTTP requests. An Origin is defined by the CORS RFC Section 3.2 as a URI scheme + hostname + port, or the special value 'null'. Default ports (HTTPS = 443, HTTP = 80) are optional here. Defaults to []. Example: https://example.com,https://sub.example.com,http://localhost:8080,http://127.0.0.1:9000","title":"CORS_WHITELIST"},{"location":"component_settings/#cors_allow_all","text":"If True, the whitelist will not be used and all origins will be accepted. Defaults to False. Useful for testing but should not be used in production.","title":"CORS_ALLOW_ALL"},{"location":"component_settings/#jwt_signing_key","text":"Holds the value of the JWT_SINGING_KEY that is used to sign JSON Web Tokens. Defaults to value of SECRET_KEY . It is useful to have a separate key if you want the possibility to invalidate all current tokens. It is recomended to set up a key rotation schedule.","title":"JWT_SIGNING_KEY"},{"location":"component_settings/#jwt_expiration_time_seconds","text":"How long a JWT should be valid in seconds. Defaults to 1200 seconds (20 min)","title":"JWT_EXPIRATION_TIME_SECONDS"},{"location":"component_settings/#allow_token_auth","text":"If True, token authentication will be allowed on the API. Defaults to True. It is recomended if you expose the API to \"normal\" users via JWT Auth to disallow Token Auth so that no endpoints are not exposed. Instead you run septarate instances for your internal applications that allows TokenAuth.","title":"ALLOW_TOKEN_AUTH"},{"location":"component_settings/#default_meter_timezone","text":"Sets the default meter timezone that is used for MeterDeviceConfig. Deaults to \"UTC\". For possible values see https://en.wikipedia.org/wiki/List_of_tz_database_time_zones All values might not be available on your system.","title":"DEFAULT_METER_TIMEZONE"},{"location":"component_settings/#dlms-udp-server","text":"","title":"DLMS UDP Server"},{"location":"component_settings/#dlms_udp_server_debug","text":"Will set the loglevel to debug. Defaults to False","title":"DLMS_UDP_SERVER_DEBUG"},{"location":"component_settings/#amqp_connection_string_1","text":"AMQP Connection string to RabbitMQ broker. Example: amqp://guest:guest@rabbitmq:5672// .","title":"AMQP_CONNECTION_STRING"},{"location":"component_settings/#amqp_exchange_name","text":"The exchange where messages will be published. Defaults to utilitarian","title":"AMQP_EXCHANGE_NAME"},{"location":"component_settings/#amqp_default_queue_name","text":"To ensure that the broker always have a queue that will receive our messages we declare in both publishers and subscribers. This is the name of the queue. Defaults to utilitarian.dlms_push_messages","title":"AMQP_DEFAULT_QUEUE_NAME"},{"location":"component_settings/#amqp_default_queue_routing_key","text":"The routing key we want to bind to the default key. Defaults to new_dlms_push_message.#\"`","title":"AMQP_DEFAULT_QUEUE_ROUTING_KEY"},{"location":"component_settings/#utilitarian_application_context","text":"Since there are a few different variations (companion standards) to DLMS/COSEM and some implements some things differntly we add the application context of receiving server to add it to any outgoing messages. You should run a separate instance of the UDP server for each different application context. It is not possible to see in the messages what companion standard the meter is using so we separate them by setting them up against different server. Defaults to units11291","title":"UTILITARIAN_APPLICATION_CONTEXT"},{"location":"component_settings/#dlms-processor","text":"","title":"DLMS PROCESSOR"},{"location":"component_settings/#dlms_consumer_debug","text":"Will set the loglevel to debug. Defaults to False","title":"DLMS_CONSUMER_DEBUG"},{"location":"component_settings/#amqp_connection_string_2","text":"AMQP Connection string to RabbitMQ broker. Example: amqp://guest:guest@rabbitmq:5672// .","title":"AMQP_CONNECTION_STRING"},{"location":"component_settings/#dlms_consumer_consume_from","text":"The queue from where to consume messages. Defaults to utilitarian.dlms_push_messages","title":"DLMS_CONSUMER_CONSUME_FROM"},{"location":"component_settings/#dlms_consumer_prefetch_count","text":"The consumer prefetch count, how many unacknowledged messages the consumer is allowed to have in memory. Default to 100","title":"DLMS_CONSUMER_PREFETCH_COUNT"},{"location":"component_settings/#dlms_consumer_publish_to","text":"Exchange to publish meter readings to. Defaults to utilitarian","title":"DLMS_CONSUMER_PUBLISH_TO"},{"location":"component_settings/#utilitarian_base_url","text":"The base url for Utilitarian API. Example: https://utilitarian.example.com:8000","title":"UTILITARIAN_BASE_URL"},{"location":"component_settings/#utilitarian_auth_token","text":"Token for authentication to Utilitarian API. Example: 9c72cba03f4920dcb4b62c4d2723fe5718990024","title":"UTILITARIAN_AUTH_TOKEN"},{"location":"component_settings/#utilitarian_request_timeout","text":"Request timeout for HTTP calls to Utilitarian API. IF you start seeing a high rate of TimeoutErrors your database might not have enough resources or you need to load balance the API. But increasing the timeout could solve you problems short-term. Defined in seconds Defaults to 15","title":"UTILITARIAN_REQUEST_TIMEOUT"},{"location":"component_settings/#utilitarian-poster","text":"","title":"Utilitarian Poster"},{"location":"component_settings/#poster_debug","text":"Enables debug. Sets loglevel to debug Defaults to False","title":"POSTER_DEBUG"},{"location":"component_settings/#amqp_connection_string_3","text":"AMQP Connection string to RabbitMQ broker. Example: amqp://guest:guest@rabbitmq:5672// .","title":"AMQP_CONNECTION_STRING"},{"location":"component_settings/#poster_consume_from","text":"Name of the queues to consume from. Defaults to utilitarian.new_meter_readings","title":"POSTER_CONSUME_FROM"},{"location":"component_settings/#poster_prefetch_count","text":"The consumer prefetch count, how many unacknowledged messages the consumer is allowed to have in memory. Default to 100","title":"POSTER_PREFETCH_COUNT"},{"location":"component_settings/#utilitarian_base_url_1","text":"The base url for Utilitarian API. Example: https://utilitarian.example.com:8000","title":"UTILITARIAN_BASE_URL"},{"location":"component_settings/#utilitarian_auth_token_1","text":"Token for authentication to Utilitarian API. Example: 9c72cba03f4920dcb4b62c4d2723fe5718990024","title":"UTILITARIAN_AUTH_TOKEN"},{"location":"component_settings/#utilitarian_request_timeout_1","text":"Request timeout for HTTP calls to Utilitarian API. IF you start seeing a high rate of TimeoutErrors your database might not have enough resources or you need to load balance the API. But increasing the timeout could solve you problems short-term. Defined in seconds Defaults to 15","title":"UTILITARIAN_REQUEST_TIMEOUT"},{"location":"components/","text":"Components in Utilitarian # Utilitarian API # The main application for Utilitarian. It stores all data and handles all scheduling of reading jobs. We provide a REST API to manage the data and jobs. docker pull quay.io/pwit/utilitarian:version See component settings on how to configure Utilitarian API DLMS UDP Server # A high throughput UDP server to receive DLMS DataNotifications from meters. docker pull quay.io/pwit/utilitarian-dlms-udp-server:version DLMS Processor # Many DLMS push messages are encrypted and to decrypt UDP messages in the receiving server would reduce throughput and we could loose messages if the server is not available to process them. Instead the UDP server sends the DLMS messages to the message broker and they are consumed by the DLMS Processor where they get decrypted and parsed into Meter Readings that is published back to the message broker. docker pull quay.io/pwit/utilitarian-dlms-processor:version Utilitarian Poster # The Utilitarian poster is a queue consumer that will receive all data that are to be saved in the Utilitarian API and send it over HTTP to the correct endpoint. docker pull quay.io/pwit/utilitarian-poster:version Database # Utilitarian uses PostgreSQL as database. Message Broker # Utilitarian uses RabbitMQ as message broker.","title":"Components"},{"location":"components/#components-in-utilitarian","text":"","title":"Components in Utilitarian"},{"location":"components/#utilitarian-api","text":"The main application for Utilitarian. It stores all data and handles all scheduling of reading jobs. We provide a REST API to manage the data and jobs. docker pull quay.io/pwit/utilitarian:version See component settings on how to configure Utilitarian API","title":"Utilitarian API"},{"location":"components/#dlms-udp-server","text":"A high throughput UDP server to receive DLMS DataNotifications from meters. docker pull quay.io/pwit/utilitarian-dlms-udp-server:version","title":"DLMS UDP Server"},{"location":"components/#dlms-processor","text":"Many DLMS push messages are encrypted and to decrypt UDP messages in the receiving server would reduce throughput and we could loose messages if the server is not available to process them. Instead the UDP server sends the DLMS messages to the message broker and they are consumed by the DLMS Processor where they get decrypted and parsed into Meter Readings that is published back to the message broker. docker pull quay.io/pwit/utilitarian-dlms-processor:version","title":"DLMS Processor"},{"location":"components/#utilitarian-poster","text":"The Utilitarian poster is a queue consumer that will receive all data that are to be saved in the Utilitarian API and send it over HTTP to the correct endpoint. docker pull quay.io/pwit/utilitarian-poster:version","title":"Utilitarian Poster"},{"location":"components/#database","text":"Utilitarian uses PostgreSQL as database.","title":"Database"},{"location":"components/#message-broker","text":"Utilitarian uses RabbitMQ as message broker.","title":"Message Broker"},{"location":"docker_compose/","text":"Setting up with docker-compose # Install docker-compose # Easiest is to use pip pip install docker-compose Define docker-compose file # Create a new docker-compose.yaml file touch docker-compose.yaml Define all services with environment variables and volumes if needed. Note We are using docker containers for Postgres and RabbitMQ just to show the system as a whole. You don't need to run them in containers, as we except just the connection string as input to the other services. # Example docker compose containers for Utiltarian version : '3' services : rabbitmq : image : rabbitmq:3-management restart : unless-stopped ports : - \"5672:5672\" - \"15672:15672\" postgres : image : postgres:10.7-apline restart : unless-stopped ports : - \"15432:5432\" volumes : - postgres_data:/var/lib/postgresql/data/ utilitarian-api : image : quay.io/pwit/utilitarian:vx.x.x restart : unless-stopped ports : - \"8000:8000\" depends_on : - rabbitmq - postgres environment : &utilitarian_env # Debug # SECURITY WARNING: Don't use in production - UTILITARIAN_DEBUG=false - UTILITARIAN_LOGLEVEL=debug - # General settings - DATABASE_URL=postgres://postgres/dbname - SECRET_KEY=verysecretkey - ALLOWED_HOSTS=utilitarian-api - AMQP_CONNECTION_STRING=amqp://guest:guest@rabbitmq:5672/ utilitarian-worker : image : quay.io/pwit/utilitarian:version restart : unless-stopped command : celery -A utilitarian worker --loglevel=INFO environment : *utilitarian_env # anchor it to api to only define once. utilitarian-beat : image : quay.io/pwit/utilitarian:version restart : unless-stopped command : celery -A utilitarian beat --loglevel=INFO environment : *utilitarian_env # anchor it to api to only define once. dlms-processor : image : quay.io/pwit/utilitarian-dlms-processor:vX.X.X restart : unless-stopped depends_on : - rabbitmq - utilitarian-api environment : - DLMS_CONSUMER_DEBUG=false - AMQP_CONNECTION_STRING=amqp://guest:guest@rabbitmq:5672/ - DLMS_CONSUMER_CONSUME_FROM=utilitarian.dlms_push_messages - DLMS_CONSUMER_PREFETCH_COUNT=100 - DLMS_CONSUMER_PUBLISH_TO=utilitarian - UTILITARIAN_BASE_URL=http://utilitarian-api:8000 - UTILITARIAN_AUTH_TOKEN=024281a8c6a12b5fb5a8445439bb9236555975fe - UTILITARIAN_REQUEST_TIMEOUT=15 dlms-udp-server : image : quay.io/pwit/utilitarian-dlms-udp-server:vX.X.X restart : unless-stopped ports : # You need to specify that it is an UDP port and not TCP! - '4059:4059/udp' depends_on : - rabbitmq environment : - DLMS_UDP_SERVER_DEBUG=false - AMQP_CONNECTION_STRING=amqp://guest:guest@rabbitmq:5672/ - AMQP_EXCHANGE_NAME=utilitarian - AMQP_DEFAULT_QUEUE_NAME=utilitarian.dlms_push_messages - AMQP_DEFAULT_QUEUE_ROUTING_KEY=\"new_dlms_push_message.#\" - UTILITARIAN_APPLICATION_CONTEXT=units11291 utilitarian-poster : image : quay.io/pwit/utilitarian-poster:vX.X.X restart : unless-stopped depends_on : - rabbitmq - utilitarian-api environment : - POSTER_DEBUG=false - AMQP_CONNECTION_STRING=amqp://guest:guest@rabbitmq:5672/ - POSTER_CONSUME_FROM=utilitarian.new_meter_readings - POSTER_PREFETCH_COUNT=100 - UTILITARIAN_BASE_URL=http://utilitarian-api:8000 - UTILITARIAN_AUTH_TOKEN=024281a8c6a12b5fb5a8445439bb9236555975fe - UTILITARIAN_REQUEST_TIMEOUT=15 volumes : - postgres_data : Note Do not name your service names using underscores. For example the host for making HTTP requests will become something_something and having underscore in HTTP_HOST header is not valid according to RFC 1034/1035. It will result in an error.","title":"Setup using Docker Compose"},{"location":"docker_compose/#setting-up-with-docker-compose","text":"","title":"Setting up with docker-compose"},{"location":"docker_compose/#install-docker-compose","text":"Easiest is to use pip pip install docker-compose","title":"Install docker-compose"},{"location":"docker_compose/#define-docker-compose-file","text":"Create a new docker-compose.yaml file touch docker-compose.yaml Define all services with environment variables and volumes if needed. Note We are using docker containers for Postgres and RabbitMQ just to show the system as a whole. You don't need to run them in containers, as we except just the connection string as input to the other services. # Example docker compose containers for Utiltarian version : '3' services : rabbitmq : image : rabbitmq:3-management restart : unless-stopped ports : - \"5672:5672\" - \"15672:15672\" postgres : image : postgres:10.7-apline restart : unless-stopped ports : - \"15432:5432\" volumes : - postgres_data:/var/lib/postgresql/data/ utilitarian-api : image : quay.io/pwit/utilitarian:vx.x.x restart : unless-stopped ports : - \"8000:8000\" depends_on : - rabbitmq - postgres environment : &utilitarian_env # Debug # SECURITY WARNING: Don't use in production - UTILITARIAN_DEBUG=false - UTILITARIAN_LOGLEVEL=debug - # General settings - DATABASE_URL=postgres://postgres/dbname - SECRET_KEY=verysecretkey - ALLOWED_HOSTS=utilitarian-api - AMQP_CONNECTION_STRING=amqp://guest:guest@rabbitmq:5672/ utilitarian-worker : image : quay.io/pwit/utilitarian:version restart : unless-stopped command : celery -A utilitarian worker --loglevel=INFO environment : *utilitarian_env # anchor it to api to only define once. utilitarian-beat : image : quay.io/pwit/utilitarian:version restart : unless-stopped command : celery -A utilitarian beat --loglevel=INFO environment : *utilitarian_env # anchor it to api to only define once. dlms-processor : image : quay.io/pwit/utilitarian-dlms-processor:vX.X.X restart : unless-stopped depends_on : - rabbitmq - utilitarian-api environment : - DLMS_CONSUMER_DEBUG=false - AMQP_CONNECTION_STRING=amqp://guest:guest@rabbitmq:5672/ - DLMS_CONSUMER_CONSUME_FROM=utilitarian.dlms_push_messages - DLMS_CONSUMER_PREFETCH_COUNT=100 - DLMS_CONSUMER_PUBLISH_TO=utilitarian - UTILITARIAN_BASE_URL=http://utilitarian-api:8000 - UTILITARIAN_AUTH_TOKEN=024281a8c6a12b5fb5a8445439bb9236555975fe - UTILITARIAN_REQUEST_TIMEOUT=15 dlms-udp-server : image : quay.io/pwit/utilitarian-dlms-udp-server:vX.X.X restart : unless-stopped ports : # You need to specify that it is an UDP port and not TCP! - '4059:4059/udp' depends_on : - rabbitmq environment : - DLMS_UDP_SERVER_DEBUG=false - AMQP_CONNECTION_STRING=amqp://guest:guest@rabbitmq:5672/ - AMQP_EXCHANGE_NAME=utilitarian - AMQP_DEFAULT_QUEUE_NAME=utilitarian.dlms_push_messages - AMQP_DEFAULT_QUEUE_ROUTING_KEY=\"new_dlms_push_message.#\" - UTILITARIAN_APPLICATION_CONTEXT=units11291 utilitarian-poster : image : quay.io/pwit/utilitarian-poster:vX.X.X restart : unless-stopped depends_on : - rabbitmq - utilitarian-api environment : - POSTER_DEBUG=false - AMQP_CONNECTION_STRING=amqp://guest:guest@rabbitmq:5672/ - POSTER_CONSUME_FROM=utilitarian.new_meter_readings - POSTER_PREFETCH_COUNT=100 - UTILITARIAN_BASE_URL=http://utilitarian-api:8000 - UTILITARIAN_AUTH_TOKEN=024281a8c6a12b5fb5a8445439bb9236555975fe - UTILITARIAN_REQUEST_TIMEOUT=15 volumes : - postgres_data : Note Do not name your service names using underscores. For example the host for making HTTP requests will become something_something and having underscore in HTTP_HOST header is not valid according to RFC 1034/1035. It will result in an error.","title":"Define docker-compose file"},{"location":"installation/","text":"On Premise # How to install and run Utilitarian Requirements # Utilitarian depends on the following systems: Postgres 10 RabbitMQ Docker # All Utilitarian components are distributed via docker images. When you have purchased a Utilitarian License you will get a login account to be able to fetch the images via docker-cli . As of now all PWIT docker images are stored in Quay and you will need to use the credentials supplied by us to log in and pull the images. $ docker login quay.io Login against server at https://quay.io/v1/ Username: pwitab+yourpullaccount Password: ThePasswordGivenToYouFromPWIT Email: any@example.com Once you have logged in you can pull the images. docker pull quay.io/pwit/repo-name:version Installation # We will provide information on how to install and run the applications via docker-compose . In the future we will also provide instructions for Kubernetes. Docker compose might not be the right tool to for you depending on your installation and system requirements but it is a good starting point and you will get a lot of insight on how to run the systems via this instruction. If you need assistance running Utilitarian in another environment please contact us . Postgres # We use Postgres as our database of choice. We recommend that you set up Postgres on a dedicated server or use a managed service for it. You can find out a lot about running Postgres online . Also make sure you have proper security settings and follow best practices. If you are running Postgres yourself you also want to set up a stable backup function . After you have set up postgres you will supply the connection string in the format: postgres://[user[:password]@][host][:port][,...][/dbname][?param1=value1&...] # Example postgres://myuser:mysecretpassword@localhost:5432/db_name Postgres is available as a docker container and if you also want to run Postgres via docker make sure you set up a volume for it so that you don't loose any of your data. docker pull postgres:10.7-alpine RabbitMQ # RabbitMQ is a message broker for the AMQP protocol. Utilitarian uses it for inter-application communication and data processing. Checkout the official RabbitMQ site to learn how to run RabbitMQ or purchase a managed solution from, for example CloudAMQP . You will need to provide a similar connection string for RabbitMQ as for Postgres: amqp://[user[:password]@][host][:port][/vhost]] # Example amqp://guest:guest@localhost:5672// RabbitMQ can also be run as a container. Just be sure to add a volume for it so you don't loose any data and we recommend that you include the management plugin so that you have an interface for management and debugging if something is not working correctly. docker pull rabbitmq:3.7.13-management-alpine Hardware # We don\u2019t have any real numbers to tell you what kind of hardware you\u2019re going to need, but we\u2019ll help you make your decision based on existing usage from real customers. It all depends on how many meters you are managing in Utilitarian and how much data you collect from them. If you have a wide range of different meters you must also take into consideration that you will have multiple services of different kinds running in your environment. We provide means to scale horizontally but the database is still the single point of failure. Since you are free to set up the database the way your IT department or Ops team requires we leave this implementation detail to the customer. Monitoring # We do not supply a standard solution to monitor the applications. All logs from the different applications are outputed on stdout via docker. So they are accessible via docker logs. You can then hook up the docker logs to different logging drivers, see the official docker documentation . There are several solutions to application and log monitoring. For example Elastic Stack or Splunk. Contact us if you want help setting this up for your installation. We are exposing a healthcheck endpoint at /_health/ that will return 200 OK if the the API is working correctly. The healthcheck can be disabled using the env variable HEALTHCHECK_DISABLED Initial setup Utilitarian API # Generate application SECRET_KEY # Utilitarian needs a SECRET_KEY for cryptographic algorithms and signing. We recommend at least 50 chars. # Example generating SECRET_KEY with openssl openssl rand -base64 50 >> CZKcDahINwxy+eYzMItr+NZRqrONvMmJ12g+hF+W/QOMh/A4tRxhQ3onvRjJv47l Set the SECRET_KEY environment variable to this key. Migrate the database # The first time you set up Utilitarian API you will need to migrate the database so all tables are present. Assuming you have started all services via docker-compose run the following command: docker-compose exec utilitarian-api sh -c \"python manage.py migrate\" Create Superuser # Assuming you have started all services via docker-compose you should run the following command docker-compose exec utilitarian-api sh -c \"python manage.py createsuperuser\" Collect static assets # Assuming you have started all services via docker-compose you should run the following command docker-compose exec utilitarian-api sh -c \"python manage.py collectstatic --no-input\" Compress static assets # Assuming you have started all services via docker-compose you should run the following command docker-compose exec utilitarian-api sh -c \"python manage.py compress\"","title":"Installation"},{"location":"installation/#on-premise","text":"How to install and run Utilitarian","title":"On Premise"},{"location":"installation/#requirements","text":"Utilitarian depends on the following systems: Postgres 10 RabbitMQ","title":"Requirements"},{"location":"installation/#docker","text":"All Utilitarian components are distributed via docker images. When you have purchased a Utilitarian License you will get a login account to be able to fetch the images via docker-cli . As of now all PWIT docker images are stored in Quay and you will need to use the credentials supplied by us to log in and pull the images. $ docker login quay.io Login against server at https://quay.io/v1/ Username: pwitab+yourpullaccount Password: ThePasswordGivenToYouFromPWIT Email: any@example.com Once you have logged in you can pull the images. docker pull quay.io/pwit/repo-name:version","title":"Docker"},{"location":"installation/#installation","text":"We will provide information on how to install and run the applications via docker-compose . In the future we will also provide instructions for Kubernetes. Docker compose might not be the right tool to for you depending on your installation and system requirements but it is a good starting point and you will get a lot of insight on how to run the systems via this instruction. If you need assistance running Utilitarian in another environment please contact us .","title":"Installation"},{"location":"installation/#postgres","text":"We use Postgres as our database of choice. We recommend that you set up Postgres on a dedicated server or use a managed service for it. You can find out a lot about running Postgres online . Also make sure you have proper security settings and follow best practices. If you are running Postgres yourself you also want to set up a stable backup function . After you have set up postgres you will supply the connection string in the format: postgres://[user[:password]@][host][:port][,...][/dbname][?param1=value1&...] # Example postgres://myuser:mysecretpassword@localhost:5432/db_name Postgres is available as a docker container and if you also want to run Postgres via docker make sure you set up a volume for it so that you don't loose any of your data. docker pull postgres:10.7-alpine","title":"Postgres"},{"location":"installation/#rabbitmq","text":"RabbitMQ is a message broker for the AMQP protocol. Utilitarian uses it for inter-application communication and data processing. Checkout the official RabbitMQ site to learn how to run RabbitMQ or purchase a managed solution from, for example CloudAMQP . You will need to provide a similar connection string for RabbitMQ as for Postgres: amqp://[user[:password]@][host][:port][/vhost]] # Example amqp://guest:guest@localhost:5672// RabbitMQ can also be run as a container. Just be sure to add a volume for it so you don't loose any data and we recommend that you include the management plugin so that you have an interface for management and debugging if something is not working correctly. docker pull rabbitmq:3.7.13-management-alpine","title":"RabbitMQ"},{"location":"installation/#hardware","text":"We don\u2019t have any real numbers to tell you what kind of hardware you\u2019re going to need, but we\u2019ll help you make your decision based on existing usage from real customers. It all depends on how many meters you are managing in Utilitarian and how much data you collect from them. If you have a wide range of different meters you must also take into consideration that you will have multiple services of different kinds running in your environment. We provide means to scale horizontally but the database is still the single point of failure. Since you are free to set up the database the way your IT department or Ops team requires we leave this implementation detail to the customer.","title":"Hardware"},{"location":"installation/#monitoring","text":"We do not supply a standard solution to monitor the applications. All logs from the different applications are outputed on stdout via docker. So they are accessible via docker logs. You can then hook up the docker logs to different logging drivers, see the official docker documentation . There are several solutions to application and log monitoring. For example Elastic Stack or Splunk. Contact us if you want help setting this up for your installation. We are exposing a healthcheck endpoint at /_health/ that will return 200 OK if the the API is working correctly. The healthcheck can be disabled using the env variable HEALTHCHECK_DISABLED","title":"Monitoring"},{"location":"installation/#initial-setup-utilitarian-api","text":"","title":"Initial setup Utilitarian API"},{"location":"installation/#generate-application-secret_key","text":"Utilitarian needs a SECRET_KEY for cryptographic algorithms and signing. We recommend at least 50 chars. # Example generating SECRET_KEY with openssl openssl rand -base64 50 >> CZKcDahINwxy+eYzMItr+NZRqrONvMmJ12g+hF+W/QOMh/A4tRxhQ3onvRjJv47l Set the SECRET_KEY environment variable to this key.","title":"Generate application SECRET_KEY"},{"location":"installation/#migrate-the-database","text":"The first time you set up Utilitarian API you will need to migrate the database so all tables are present. Assuming you have started all services via docker-compose run the following command: docker-compose exec utilitarian-api sh -c \"python manage.py migrate\"","title":"Migrate the database"},{"location":"installation/#create-superuser","text":"Assuming you have started all services via docker-compose you should run the following command docker-compose exec utilitarian-api sh -c \"python manage.py createsuperuser\"","title":"Create Superuser"},{"location":"installation/#collect-static-assets","text":"Assuming you have started all services via docker-compose you should run the following command docker-compose exec utilitarian-api sh -c \"python manage.py collectstatic --no-input\"","title":"Collect static assets"},{"location":"installation/#compress-static-assets","text":"Assuming you have started all services via docker-compose you should run the following command docker-compose exec utilitarian-api sh -c \"python manage.py compress\"","title":"Compress static assets"},{"location":"open_source/","text":"Open Source # We want Utilitarian to be a simple companion to our customers in their AMR operations. We provide documented interfaces and APIs to our services and messaging system. So you have the freedom of making use of the data as you see fit. For example if you need to make a special analytics service for some customers you can either get the data from the Utilitarian API or subscribe directly to it via the message broker. We provide some helpers to write your own integrations written in Python but since we are using AMQP and HTTP you can find implementations of those protocols in many programing languages and use the documentation in our helpers to speed up development of your own integrations in your language of choice. We also provide many libraries of the protocols we support as open source. Utilitarian Helpers # Utilitarian Queue Consumer # Check it out on GitHub: https://github.com/pwitab/utilitarian-queue-consumer The Utilitarian Queue Consumer is a micro framework to write consumers for the messages in Utilitarian. It gives you good default for consuming messages and producing new messages. AMR UM # Check it out on GitHub: https://github.com/pwitab/amr-um AMR UM (Unified Messaging for Automatic Meter Readings) is our attempt to have standard and well documented messaging framwork. In the repo you can find Python helpers to format the data and documentation on the different schemas used. Protocols # DLMS/COSEM # Check it out on GitHub: https://github.com/pwitab/dlms-cosem DLMS/COSEM (IEC 62056, EN13757-1) is the global standard for smart energy metering, control and management. It specifies an object-oriented data model, an application layer protocol and media-specific communication profiles. IEC 62056-21 # Check it out on GitHub: https://github.com/pwitab/iec62056-21 IEC 62056-21 (earlier IEC 61107 or sometimes just IEC 1107, is an international standard for a computer protocol to read utility meters. It is designed to operate over any media, including the Internet. A meter sends ASCII (in modes A..D) or HDLC (mode E) data to a nearby hand-held unit (HHU) using a serial port. The physical media are usually either modulated light, sent with an LED and received with a photodiode, or a pair of wires, usually modulated by a 20mA current loop. The protocol is usually half-duplex. I-FLAG / Corus # Check it out on GitHub: https://github.com/pwitab/iflag The Corus / I-FLAG protocol is used in Actaris / Itron meters. It is a propriotary protocol that have some elements from IEC-62056-21 but is using binary data of special formats to read out data.","title":"Open Source"},{"location":"open_source/#open-source","text":"We want Utilitarian to be a simple companion to our customers in their AMR operations. We provide documented interfaces and APIs to our services and messaging system. So you have the freedom of making use of the data as you see fit. For example if you need to make a special analytics service for some customers you can either get the data from the Utilitarian API or subscribe directly to it via the message broker. We provide some helpers to write your own integrations written in Python but since we are using AMQP and HTTP you can find implementations of those protocols in many programing languages and use the documentation in our helpers to speed up development of your own integrations in your language of choice. We also provide many libraries of the protocols we support as open source.","title":"Open Source"},{"location":"open_source/#utilitarian-helpers","text":"","title":"Utilitarian Helpers"},{"location":"open_source/#utilitarian-queue-consumer","text":"Check it out on GitHub: https://github.com/pwitab/utilitarian-queue-consumer The Utilitarian Queue Consumer is a micro framework to write consumers for the messages in Utilitarian. It gives you good default for consuming messages and producing new messages.","title":"Utilitarian Queue Consumer"},{"location":"open_source/#amr-um","text":"Check it out on GitHub: https://github.com/pwitab/amr-um AMR UM (Unified Messaging for Automatic Meter Readings) is our attempt to have standard and well documented messaging framwork. In the repo you can find Python helpers to format the data and documentation on the different schemas used.","title":"AMR UM"},{"location":"open_source/#protocols","text":"","title":"Protocols"},{"location":"open_source/#dlmscosem","text":"Check it out on GitHub: https://github.com/pwitab/dlms-cosem DLMS/COSEM (IEC 62056, EN13757-1) is the global standard for smart energy metering, control and management. It specifies an object-oriented data model, an application layer protocol and media-specific communication profiles.","title":"DLMS/COSEM"},{"location":"open_source/#iec-62056-21","text":"Check it out on GitHub: https://github.com/pwitab/iec62056-21 IEC 62056-21 (earlier IEC 61107 or sometimes just IEC 1107, is an international standard for a computer protocol to read utility meters. It is designed to operate over any media, including the Internet. A meter sends ASCII (in modes A..D) or HDLC (mode E) data to a nearby hand-held unit (HHU) using a serial port. The physical media are usually either modulated light, sent with an LED and received with a photodiode, or a pair of wires, usually modulated by a 20mA current loop. The protocol is usually half-duplex.","title":"IEC 62056-21"},{"location":"open_source/#i-flag-corus","text":"Check it out on GitHub: https://github.com/pwitab/iflag The Corus / I-FLAG protocol is used in Actaris / Itron meters. It is a propriotary protocol that have some elements from IEC-62056-21 but is using binary data of special formats to read out data.","title":"I-FLAG / Corus"},{"location":"protocols/","text":"Protocols # The different protocols we support DLMS/COSEM # DLMS/COSEM (IEC 62056, EN13757-1) is the global standard for smart energy metering, control and management. It specifies an object-oriented data model, an application layer protocol and media-specific communication profiles. We are developing an open source library for DLMS/COSEM IEC 62056-21 # IEC 62056-21 superseded IEC 61107 (sometimes just called IEC 1107). It is used for direct local data exchange. It has been designed to act as the protocol used when reading the meter via its optical port. But it is used with several medias, including sending the data over the internet. We are developing an open source library for IEC 62056-21 LIS-200 # LIS-200 is a subset of IEC62056-21. It as implemented when the IEC62056-21 didn't have all the functionality needed. It is basically the same as IEC62056-21 in the flow but object identification is done differently. Corus protocol / I-FLAG # The Corus protocol is used in Actaris / Itron meters. It is a propriotary protocol that have some elements from IEC-62056-21 but is using binary data of special formats to read out data. We are developing an open source library for I-FLAG","title":"Protocols"},{"location":"protocols/#protocols","text":"The different protocols we support","title":"Protocols"},{"location":"protocols/#dlmscosem","text":"DLMS/COSEM (IEC 62056, EN13757-1) is the global standard for smart energy metering, control and management. It specifies an object-oriented data model, an application layer protocol and media-specific communication profiles. We are developing an open source library for DLMS/COSEM","title":"DLMS/COSEM"},{"location":"protocols/#iec-62056-21","text":"IEC 62056-21 superseded IEC 61107 (sometimes just called IEC 1107). It is used for direct local data exchange. It has been designed to act as the protocol used when reading the meter via its optical port. But it is used with several medias, including sending the data over the internet. We are developing an open source library for IEC 62056-21","title":"IEC 62056-21"},{"location":"protocols/#lis-200","text":"LIS-200 is a subset of IEC62056-21. It as implemented when the IEC62056-21 didn't have all the functionality needed. It is basically the same as IEC62056-21 in the flow but object identification is done differently.","title":"LIS-200"},{"location":"protocols/#corus-protocol-i-flag","text":"The Corus protocol is used in Actaris / Itron meters. It is a propriotary protocol that have some elements from IEC-62056-21 but is using binary data of special formats to read out data. We are developing an open source library for I-FLAG","title":"Corus protocol / I-FLAG"},{"location":"pwit/","text":"About Utilitarian # Utilitarian is regitered secondary business name of Palmlund Wahlgren Innovative Technology AB. Under the name of Utilitarian we are marketing our Smart Metering and Smart Grid products and services. Utilitarian is the result of our desire to make AMR simpler for the end user and put the data in focus instead of the means to collect the data. We have used all our experience in the Smart Metering and Smart Grid field to deliver a solution that can fit any size and type of utility company. Palmlund Wahlgren Innovative Technology AB # Palmlund Wahlgren Innovative Technology AB (PWIT AB) is an engineering firm focused on delivering solutions and consultation services within the fields of automation, measurement technology and IT. By combining our strengths we have landed in the field of Internet of Things where we have been able to manage projects and build devices and software within the smart metering segment. We have been working with Swedish utility companies in local projects and EU-level projects where we have designed and implemented solutions for Real Time AMR (Automatic Meter Readings) and Next Generation MDM (Meter Data Management) using NoSQL-databases. We have taken full responsibility for the whole metering chain. From implementing protocol interpreters on embedded devices and in the Cloud to enable real-time metering on older energy meters to meter data management and integration to legacy Enterprise systems such as SAP for billing. Learn more about us","title":"About Utilitarian"},{"location":"pwit/#about-utilitarian","text":"Utilitarian is regitered secondary business name of Palmlund Wahlgren Innovative Technology AB. Under the name of Utilitarian we are marketing our Smart Metering and Smart Grid products and services. Utilitarian is the result of our desire to make AMR simpler for the end user and put the data in focus instead of the means to collect the data. We have used all our experience in the Smart Metering and Smart Grid field to deliver a solution that can fit any size and type of utility company.","title":"About Utilitarian"},{"location":"pwit/#palmlund-wahlgren-innovative-technology-ab","text":"Palmlund Wahlgren Innovative Technology AB (PWIT AB) is an engineering firm focused on delivering solutions and consultation services within the fields of automation, measurement technology and IT. By combining our strengths we have landed in the field of Internet of Things where we have been able to manage projects and build devices and software within the smart metering segment. We have been working with Swedish utility companies in local projects and EU-level projects where we have designed and implemented solutions for Real Time AMR (Automatic Meter Readings) and Next Generation MDM (Meter Data Management) using NoSQL-databases. We have taken full responsibility for the whole metering chain. From implementing protocol interpreters on embedded devices and in the Cloud to enable real-time metering on older energy meters to meter data management and integration to legacy Enterprise systems such as SAP for billing. Learn more about us","title":"Palmlund Wahlgren Innovative Technology AB"},{"location":"time/","text":"Time in Utilitarian # One of the harder things to manage in AMR Operations is time. Especially when you are developing a solution that should be able to use many different kinds of meters in different regions under different regulatory rules. An example of this is Sweden. For electricity meters the time of meters should be Swedish normal time, or Swedish time without day light savings times change. But for gas meter the gas day is counted from 06:00-06:00 in Swedish local time, with day light savings. Timestamps # To make it possible to have meters that are in different timezones all timestamps in Utilitarian has a timezone attached to it. If the timezone isn't UTC it will be transformed to UTC upon saving. Timezones. # In MeterDeviceConfig it is possible to set a timezone for a meter. If you want the meter to follow normal daylight savings rules for a timezone just use the normal one. Ex Europe/Stockholm . But if you need a meter in a zone that doesn't change at Daylight Savings Time (DST) you should use a constructed zone like Etc/GMT-2 . To learn more about available timezones visit https://en.wikipedia.org/wiki/List_of_tz_database_time_zones Warning When using constructed zones, ex Etc/GMT-2 the sign of the hour is inverted from that of the ISO 8601 standard. So Etc/GMT-2 is 2 hours ahead of GMT. read more here... Meter time # How the time is handled inside a meter device can be very different depending on manufacturer and protocol. Some meters have no knowledge of timezone, some have but have turned it off or some has detailed information about timezone and follow DST changes. We implement the time management in our execution environments and meter integrations according to the way the meter handles it internally. Syncing time # When we want to sync time we do it against the servers internal clock and timezone UTC. When time needs to be corrected in a meter we convert the server time to the local time for the meters timezone and write it to the meter. This is run as an AmrTask and can be run on a schedule that suits the application.","title":"Time in Utilitarian"},{"location":"time/#time-in-utilitarian","text":"One of the harder things to manage in AMR Operations is time. Especially when you are developing a solution that should be able to use many different kinds of meters in different regions under different regulatory rules. An example of this is Sweden. For electricity meters the time of meters should be Swedish normal time, or Swedish time without day light savings times change. But for gas meter the gas day is counted from 06:00-06:00 in Swedish local time, with day light savings.","title":"Time in Utilitarian"},{"location":"time/#timestamps","text":"To make it possible to have meters that are in different timezones all timestamps in Utilitarian has a timezone attached to it. If the timezone isn't UTC it will be transformed to UTC upon saving.","title":"Timestamps"},{"location":"time/#timezones","text":"In MeterDeviceConfig it is possible to set a timezone for a meter. If you want the meter to follow normal daylight savings rules for a timezone just use the normal one. Ex Europe/Stockholm . But if you need a meter in a zone that doesn't change at Daylight Savings Time (DST) you should use a constructed zone like Etc/GMT-2 . To learn more about available timezones visit https://en.wikipedia.org/wiki/List_of_tz_database_time_zones Warning When using constructed zones, ex Etc/GMT-2 the sign of the hour is inverted from that of the ISO 8601 standard. So Etc/GMT-2 is 2 hours ahead of GMT. read more here...","title":"Timezones."},{"location":"time/#meter-time","text":"How the time is handled inside a meter device can be very different depending on manufacturer and protocol. Some meters have no knowledge of timezone, some have but have turned it off or some has detailed information about timezone and follow DST changes. We implement the time management in our execution environments and meter integrations according to the way the meter handles it internally.","title":"Meter time"},{"location":"time/#syncing-time","text":"When we want to sync time we do it against the servers internal clock and timezone UTC. When time needs to be corrected in a meter we convert the server time to the local time for the meters timezone and write it to the meter. This is run as an AmrTask and can be run on a schedule that suits the application.","title":"Syncing time"},{"location":"upgrade_instructions/","text":"Upgrade instructions # How to upgrade Utilitarian to a newer version. Warning These instructions assume that you are able to take Utilitarian down for maintenance and allow time for the migrations to run. If you need you Utilitarian instance to be up during the migration please contact us and we will create an upgrade strategy together. Warning When upgrading Utilitarian API you might need to upgrade other Utilitarian services. If you are unsure of which services that need upgrading, please contact us. 1. Backup # Take a backup of your database and verify the backup. This can be done using pg_dump . pg_dump documentation If you have the ability to take a snapshot of your database server it is also recommended. 2. Close down services # Close down all services that depend on the API. Most edge services will keep handling messages and they will be queued up in RabbitMQ for when the services come online again. Example: Utilitarian DLMS Parser Utilitarian Poster 3. Close down Utilitarian API # Close down Utilitarian API, beat process and all worker processes. 4. Pull new versions # Pull down new image version of Utilitarian from the docker repository docker pull quay.io/pwit/utilitarian:{version} 5. Upgrade docker config # Upgrade run config, for example docker-compose files with the new image. Note You also have to remember to update the beat-process and all worker processes since they are based on the utilitarian image. 6. Start Utilitarian API # Start only the Utilitarian API processes. 7. Run migrations # Note This command can take a long time Depending on how you are running Utilitarian how you initiate the command to run migrations can be a bit different. The command is: python manage.py migrate if you are using docker-compose you should run it like: docker-compose exec utilitarian-api sh -c \"python manage.py migrate\" 8. Update static assets # Some upgrades might have changes to the static assets. It is good to run the collection and compression of static assets again. The command is: python manage.py collectstatic --no-input python manage.py compress if you are using docker-compose you should run it like: docker-compose exec utilitarian-api sh -c \"python manage.py collectstatic --no-input\" docker-compose exec utilitarian-api sh -c \"python manage.py compress\" 9. Start up all services # Now start up all remaining services to get back all functionality to Utilitarian. 10. Verify upgrade # Verify that everything seems to be correct in the application. If you find an issue please contact us.","title":"Upgrading"},{"location":"upgrade_instructions/#upgrade-instructions","text":"How to upgrade Utilitarian to a newer version. Warning These instructions assume that you are able to take Utilitarian down for maintenance and allow time for the migrations to run. If you need you Utilitarian instance to be up during the migration please contact us and we will create an upgrade strategy together. Warning When upgrading Utilitarian API you might need to upgrade other Utilitarian services. If you are unsure of which services that need upgrading, please contact us.","title":"Upgrade instructions"},{"location":"upgrade_instructions/#1-backup","text":"Take a backup of your database and verify the backup. This can be done using pg_dump . pg_dump documentation If you have the ability to take a snapshot of your database server it is also recommended.","title":"1. Backup"},{"location":"upgrade_instructions/#2-close-down-services","text":"Close down all services that depend on the API. Most edge services will keep handling messages and they will be queued up in RabbitMQ for when the services come online again. Example: Utilitarian DLMS Parser Utilitarian Poster","title":"2. Close down services"},{"location":"upgrade_instructions/#3-close-down-utilitarian-api","text":"Close down Utilitarian API, beat process and all worker processes.","title":"3. Close down Utilitarian API"},{"location":"upgrade_instructions/#4-pull-new-versions","text":"Pull down new image version of Utilitarian from the docker repository docker pull quay.io/pwit/utilitarian:{version}","title":"4. Pull new versions"},{"location":"upgrade_instructions/#5-upgrade-docker-config","text":"Upgrade run config, for example docker-compose files with the new image. Note You also have to remember to update the beat-process and all worker processes since they are based on the utilitarian image.","title":"5. Upgrade docker config"},{"location":"upgrade_instructions/#6-start-utilitarian-api","text":"Start only the Utilitarian API processes.","title":"6. Start Utilitarian API"},{"location":"upgrade_instructions/#7-run-migrations","text":"Note This command can take a long time Depending on how you are running Utilitarian how you initiate the command to run migrations can be a bit different. The command is: python manage.py migrate if you are using docker-compose you should run it like: docker-compose exec utilitarian-api sh -c \"python manage.py migrate\"","title":"7. Run migrations"},{"location":"upgrade_instructions/#8-update-static-assets","text":"Some upgrades might have changes to the static assets. It is good to run the collection and compression of static assets again. The command is: python manage.py collectstatic --no-input python manage.py compress if you are using docker-compose you should run it like: docker-compose exec utilitarian-api sh -c \"python manage.py collectstatic --no-input\" docker-compose exec utilitarian-api sh -c \"python manage.py compress\"","title":"8. Update static assets"},{"location":"upgrade_instructions/#9-start-up-all-services","text":"Now start up all remaining services to get back all functionality to Utilitarian.","title":"9. Start up all services"},{"location":"upgrade_instructions/#10-verify-upgrade","text":"Verify that everything seems to be correct in the application. If you find an issue please contact us.","title":"10. Verify upgrade"},{"location":"using_https_on_api/","text":"Run Utilitarian API under HTTPS # The main application does not provide any means to run under HTTPS. This is done via a reverse proxy like NginX or HAProxy. Having a reverse proxy also simplifies load balancing if you need to scale up the API. You can run it as a stand alone service or run it as docker container. We leave it up to the customer to define the way they want to terminate SSL/TLS and handle load balancing but will of course provide help if needed. Settings in Utilitarian API if running under HTTPS: # SESSION_COOKIE_SECURE should be set to true CSRF_COOKIE_SECURE should be set to true Depending on your proxy settings you should set the USE_X_FORWARDED_HOST , USE_X_FORWARDED_PORT and USE_X_FORWARDED_PROTO to true SECURE_HSTS_SECONDS should be set to 60 and when you have made sure it works properly it can be increased to a higher value","title":"Use HTTPS"},{"location":"using_https_on_api/#run-utilitarian-api-under-https","text":"The main application does not provide any means to run under HTTPS. This is done via a reverse proxy like NginX or HAProxy. Having a reverse proxy also simplifies load balancing if you need to scale up the API. You can run it as a stand alone service or run it as docker container. We leave it up to the customer to define the way they want to terminate SSL/TLS and handle load balancing but will of course provide help if needed.","title":"Run Utilitarian API under HTTPS"},{"location":"using_https_on_api/#settings-in-utilitarian-api-if-running-under-https","text":"SESSION_COOKIE_SECURE should be set to true CSRF_COOKIE_SECURE should be set to true Depending on your proxy settings you should set the USE_X_FORWARDED_HOST , USE_X_FORWARDED_PORT and USE_X_FORWARDED_PROTO to true SECURE_HSTS_SECONDS should be set to 60 and when you have made sure it works properly it can be increased to a higher value","title":"Settings in Utilitarian API if running under HTTPS:"},{"location":"execution_environments/execution_environments/","text":"Execution Environments. # When reading a meter in Utilitarian (Pull Architecture) we provide a number of different execution environments. These environments provide a number of methods to call on the meter that has been loaded into the environment. AmrTask are saved with knowledge of the execution environment it is supposed to run in. We are having a concept of generic environments and specific environments. A generic environment is designed so that most meters using the same technology/protocol can use the environment. But if a meter type with the same protocol does a action in a slightly different way than the normal a special specific environment is made for that meter at integration time to facilitate the different functionality","title":"Overview"},{"location":"execution_environments/execution_environments/#execution-environments","text":"When reading a meter in Utilitarian (Pull Architecture) we provide a number of different execution environments. These environments provide a number of methods to call on the meter that has been loaded into the environment. AmrTask are saved with knowledge of the execution environment it is supposed to run in. We are having a concept of generic environments and specific environments. A generic environment is designed so that most meters using the same technology/protocol can use the environment. But if a meter type with the same protocol does a action in a slightly different way than the normal a special specific environment is made for that meter at integration time to facilitate the different functionality","title":"Execution Environments."},{"location":"execution_environments/generic_corus/","text":"Generic Corus # generic_corus Execution environment name: generic_corus Function Calls # Read Parameters: read_parameters Read Database: read_database_by_date Read Database By Offset Seconds: read_database_by_offset_seconds Set Time: set_time Read Parameters # read_parameters It is possible to read several parameters in the same request. Argument Type Required Description parameters list[string] yes List of parameters to read. See parameters list. # example issuing an on demand amr task via the API { \"meter_device\" : \"890b61bb-6594-48db-ad90-584d56754455\" , \"task_call\" : \"read_parameters\" , \"task_kwargs\" : { \"parameters\" : [ \"index_converted\" , \"index_unconverted\" ], }, \"run_offset_seconds\" : 0 , \"meter_execution_type\" : \"generic_corus\" } Read database # read_database Corus databases are read from the latest value to the oldest. So start_date is the newest value and stop_date is the oldest value. If no dates are provided all data in database is read. Reading by date is useful to re-read old data. Maybe a task failed to collect them or there was a network error or device error. Argument Type Required Description start_dat datetime no The date for the oldest data to start reading from. end_date datetime no The date to stop reading. If not proved all data from the start date will be read up until the latest entry database str yes Name of database. Possible values: interval , hourly , daily , monthly . # example issuing an on demand amr task via the API { \"meter_device\" : \"890b61bb-6594-48db-ad90-584d56754455\" , \"task_call\" : \"read_database\" , \"task_kwargs\" : { \"end_date\" : \"2020-01-15T06:00:00+0100\" , \"start_date\" : \"2020-01-16T06:00:00+0100\" , \"database\" : 'interval' , }, \"run_offset_seconds\" : 0 , \"meter_execution_type\" : \"generic_corus\" } Read database by offset # read_database_by_offset_seconds Similar to reading by date but instead you use the current time and read backwards by the offset seconds. For example offset_seconds=900 would read the last 15 min in an archive. Best used with a scheduled AmrTask. Argument Type Required Description offset_seconds int yes Number of seconds to read by. database string yes Name of database. Possible values: interval , hourly , daily , monthly . Note When using with a scheduled AmrTask that also is put in an offset schedule you might need to increase the offset to get all requested values. # example issuing an on demand amr task via the API { \"meter_device\" : \"890b61bb-6594-48db-ad90-584d56754455\" , \"task_call\" : \"read_database_by_offset_seconds\" , \"task_kwargs\" : { \"offset_seconds\" : 1000 , \"database\" : 'hourly' , }, \"run_offset_seconds\" : 0 , \"meter_execution_type\" : \"generic_corus\" } Set time # set_time Will sync the time in the meter with the server. No parameters needed. Argument Type Required Description allowed_drift_seconds int yes The time in seconds the meters time is allowed to drift from the server until the time is corrected. # example issuing an on demand amr task via the API { \"meter_device\" : \"890b61bb-6594-48db-ad90-584d56754455\" , \"task_call\" : \"set_time\" , \"task_kwargs\" : { \"allowed_drift_seconds\" : 60 }, \"run_offset_seconds\" : 0 , \"meter_execution_type\" : \"generic_lis200\" } Parameters for instantaneous readout # Parameter Description index_unconverted Current unconverted index of device index_converted Current converted index of device datetime Current time in device battery_days Battery life in days input_pulse_weight The weight of input pulses parameter_mapping_version The parameter mapping of the device temperature_base Base Temperature temperature_limit_low Low threshold for temperature temperature_limit_high High threshold for temperature temperature Current gas temperature pressure_base Base pressure pressure_limit_low Low threshold for pressure pressure_limit_high High threshold for pressure pressure Current gas pressure pressure_2 Current gas pressure, secondary pressure sensor. Only on devices with secondary sensor flowrate_unconverted Current unconverted flowrate flowrate_converted Current converted flowrate compressibility_formula Code for the formula used in the device. firmware_version Firmware version of the device kernel_version Kernel version of the device. Only available on MID devices Parameters for database readout # The main difference in AMR perspective between reading databases from a non-MID devices and a MID-device is that the metering index is available in all databases on a MID devices. It is always better to collect the index and calculate the consumption instead of just collecting consumptions as loosing data will make it hard to interpolate. For reading non-MID devices on a schedule shorter than monthly we recommend that reading the metering is done via the read_parameters call on the schedule you like and complement with a monthly read of the monthly database to get a values that is guaranteed to be on the gas day change. Depending on users queue setup and number of meters there will be a delay in reading instantaneous values of the metering index. On MID-devices just read the databases as they are. If a shorter period than hourly is needed use the interval database and set it to the period you like on the device and schedule readout either quarterly or hourly. Interval # Non MID devices # Parameter Description record_duration Duration of the database record status Record status end_date End datetime of record consumption_unconverted_interval Unconverted consumption during period consumption_converted_interval Converted consumption during period consumption_unconverted_interval_under_alarm Unconverted consumption under alarm during period consumption_converted_interval_under_alarm Converted consumption under alarm during period temperature_interval_minimum Minimum temperature during period temperature_interval_maximum Maximum temperature during period temperature_interval_average Average temperature during period pressure_interval_minimum Minimum pressure during period pressure_interval_maximum Maximum pressure during period pressure_interval_average Average pressure during period flowrate_unconverted_interval_minimum Minimum unconverted flowrate during period flowrate_unconverted_interval_maximum Maximum unconverted flowrate during period flowrate_converted_interval_minimum Minimum converted flowrate during period flowrate_converted_interval_maximum Maximum converted flowrate during period flowrate_unconverted_interval_average Average unconverted flowrate during period flowrate_converted_interval_average Average converted flowrate during period start_date Start datetime of record MID Devices # All parameters from non-MID including the following: Parameter Description index_unconverted Metering index unconverted gas volume index_converted Metering index converted gas volume counter_unconverted_under_alarm Counter unconverted under alarm counter_converted_under_alarm Counted converted under alarm pressure_2_interval_minimum Minimum pressure during period, secondary sensor. Only available on devices with a secondary pressure sensor pressure_2_interval_maximum Maximum pressure during period, secondary sensor. Only available on devices with a secondary pressure sensor pressure_2_interval_average Average pressure during period, secondary sensor. Only available on devices with a secondary pressure sensor Hourly # Non MID devices # Parameter Description record_duration Duration of the database record status Record status end_date End datetime of record consumption_unconverted_hourly Unconverted consumption during period consumption_converted_hourly Converted consumption during period consumption_unconverted_hourly_under_alarm Unconverted consumption under alarm during period consumption_converted_hourly_under_alarm Converted consumption under alarm during period temperature_hourly_minimum Minimum temperature during period temperature_hourly_maximum Maximum temperature during period temperature_hourly_average Average temperature during period pressure_hourly_minimum Minimum pressure during period pressure_hourly_maximum Maximum pressure during period pressure_hourly_average Average pressure during period flowrate_unconverted_hourly_minimum Minimum unconverted flowrate during period flowrate_unconverted_hourly_maximum Maximum unconverted flowrate during period flowrate_converted_hourly_minimum Minimum converted flowrate during period flowrate_converted_hourly_maximum Maximum converted flowrate during period flowrate_unconverted_hourly_average Average unconverted flowrate during period flowrate_converted_hourly_average Average converted flowrate during period start_date Start datetime of record MID Devices # All parameters from non-MID including the following: Parameter Description index_unconverted Metering index unconverted gas volume index_converted Metering index converted gas volume counter_unconverted_under_alarm Counter unconverted under alarm counter_converted_under_alarm Counted converted under alarm pressure_2_hourly_minimum Minimum pressure during period, secondary sensor. Only available on devices with a secondary pressure sensor pressure_2_hourly_maximum Maximum pressure during period, secondary sensor. Only available on devices with a secondary pressure sensor pressure_2_hourly_average Average pressure during period, secondary sensor. Only available on devices with a secondary pressure sensor Daily # Non MID devices # Parameter Description record_duration Duration of the database record status Record status end_date End datetime of record consumption_unconverted_daily Unconverted consumption during period consumption_converted_daily Converted consumption during period consumption_unconverted_daily_under_alarm Unconverted consumption under alarm during period consumption_converted_daily_under_alarm Converted consumption under alarm during period temperature_daily_minimum Minimum temperature during period temperature_daily_maximum Maximum temperature during period temperature_daily_average Average temperature during period pressure_daily_minimum Minimum pressure during period pressure_daily_maximum Maximum pressure during period pressure_daily_average Average pressure during period flowrate_unconverted_daily_minimum Minimum unconverted flowrate during period flowrate_unconverted_daily_maximum Maximum unconverted flowrate during period flowrate_converted_daily_minimum Minimum converted flowrate during period flowrate_converted_daily_maximum Maximum converted flowrate during period flowrate_unconverted_daily_average Average unconverted flowrate during period flowrate_converted_daily_average Average converted flowrate during period start_date Start datetime of record MID Devices # All parameters from non-MID including the following: Parameter Description index_unconverted Metering index unconverted gas volume index_converted Metering index converted gas volume counter_unconverted_under_alarm Counter unconverted under alarm counter_converted_under_alarm Counted converted under alarm pressure_2_daily_minimum Minimum pressure during period, secondary sensor. Only available on devices with a secondary pressure sensor pressure_2_daily_maximum Maximum pressure during period, secondary sensor. Only available on devices with a secondary pressure sensor pressure_2_daily_average Average pressure during period, secondary sensor. Only available on devices with a secondary pressure sensor Monthly # Non MID devices # Parameter Description record_duration Duration of the database record status Record status end_date End datetime of record consumption_unconverted_monthly Unconverted consumption during period consumption_converted_monthly Converted consumption during period consumption_unconverted_monthly_under_alarm Unconverted consumption under alarm during period consumption_converted_monthly_under_alarm Converted consumption under alarm during period temperature_monthly_minimum Minimum temperature during period temperature_monthly_maximum Maximum temperature during period temperature_monthly_average Average temperature during period pressure_monthly_minimum Minimum pressure during period pressure_monthly_maximum Maximum pressure during period pressure_monthly_average Average pressure during period flowrate_unconverted_monthly_minimum Minimum unconverted flowrate during period flowrate_unconverted_monthly_maximum Maximum unconverted flowrate during period flowrate_converted_monthly_minimum Minimum converted flowrate during period flowrate_converted_monthly_maximum Maximum converted flowrate during period index_unconverted Metering index unconverted gas volume at end_date of record index_converted Metering index converted gas volume at end_date of record counter_unconverted_under_alarm Counter unconverted under alarm at end_date of record counter_converted_under_alarm Counter converted under alarm at end_date of record consumption_unconverted_monthly_maximum Maximum unconverted consumption of interval period during month consumption_unconverted_monthly_maximum_date Date when maximum unconverted consumption was registered. consumption_converted_monthly_maximum Maximum converted consumption of interval period during month consumption_converted_monthly_maximum_date Date when maximum converted consumption was registered. flowrate_unconverted_monthly_average Average unconverted flowrate during period flowrate_converted_monthly_average Average converted flowrate during period start_date Start datetime of record MID Devices # All parameters from non-MID including the following: Parameter Description pressure_2_monthly_minimum Minimum pressure during period, secondary sensor. Only available on devices with a secondary pressure sensor pressure_2_monthly_maximum Maximum pressure during period, secondary sensor. Only available on devices with a secondary pressure sensor pressure_2_monthly_average Average pressure during period, secondary sensor. Only available on devices with a secondary pressure sensor","title":"Generic Corus"},{"location":"execution_environments/generic_corus/#generic-corus","text":"generic_corus Execution environment name: generic_corus","title":"Generic Corus"},{"location":"execution_environments/generic_corus/#function-calls","text":"Read Parameters: read_parameters Read Database: read_database_by_date Read Database By Offset Seconds: read_database_by_offset_seconds Set Time: set_time","title":"Function Calls"},{"location":"execution_environments/generic_corus/#read-parameters","text":"read_parameters It is possible to read several parameters in the same request. Argument Type Required Description parameters list[string] yes List of parameters to read. See parameters list. # example issuing an on demand amr task via the API { \"meter_device\" : \"890b61bb-6594-48db-ad90-584d56754455\" , \"task_call\" : \"read_parameters\" , \"task_kwargs\" : { \"parameters\" : [ \"index_converted\" , \"index_unconverted\" ], }, \"run_offset_seconds\" : 0 , \"meter_execution_type\" : \"generic_corus\" }","title":"Read Parameters"},{"location":"execution_environments/generic_corus/#read-database","text":"read_database Corus databases are read from the latest value to the oldest. So start_date is the newest value and stop_date is the oldest value. If no dates are provided all data in database is read. Reading by date is useful to re-read old data. Maybe a task failed to collect them or there was a network error or device error. Argument Type Required Description start_dat datetime no The date for the oldest data to start reading from. end_date datetime no The date to stop reading. If not proved all data from the start date will be read up until the latest entry database str yes Name of database. Possible values: interval , hourly , daily , monthly . # example issuing an on demand amr task via the API { \"meter_device\" : \"890b61bb-6594-48db-ad90-584d56754455\" , \"task_call\" : \"read_database\" , \"task_kwargs\" : { \"end_date\" : \"2020-01-15T06:00:00+0100\" , \"start_date\" : \"2020-01-16T06:00:00+0100\" , \"database\" : 'interval' , }, \"run_offset_seconds\" : 0 , \"meter_execution_type\" : \"generic_corus\" }","title":"Read database"},{"location":"execution_environments/generic_corus/#read-database-by-offset","text":"read_database_by_offset_seconds Similar to reading by date but instead you use the current time and read backwards by the offset seconds. For example offset_seconds=900 would read the last 15 min in an archive. Best used with a scheduled AmrTask. Argument Type Required Description offset_seconds int yes Number of seconds to read by. database string yes Name of database. Possible values: interval , hourly , daily , monthly . Note When using with a scheduled AmrTask that also is put in an offset schedule you might need to increase the offset to get all requested values. # example issuing an on demand amr task via the API { \"meter_device\" : \"890b61bb-6594-48db-ad90-584d56754455\" , \"task_call\" : \"read_database_by_offset_seconds\" , \"task_kwargs\" : { \"offset_seconds\" : 1000 , \"database\" : 'hourly' , }, \"run_offset_seconds\" : 0 , \"meter_execution_type\" : \"generic_corus\" }","title":"Read database by offset"},{"location":"execution_environments/generic_corus/#set-time","text":"set_time Will sync the time in the meter with the server. No parameters needed. Argument Type Required Description allowed_drift_seconds int yes The time in seconds the meters time is allowed to drift from the server until the time is corrected. # example issuing an on demand amr task via the API { \"meter_device\" : \"890b61bb-6594-48db-ad90-584d56754455\" , \"task_call\" : \"set_time\" , \"task_kwargs\" : { \"allowed_drift_seconds\" : 60 }, \"run_offset_seconds\" : 0 , \"meter_execution_type\" : \"generic_lis200\" }","title":"Set time"},{"location":"execution_environments/generic_corus/#parameters-for-instantaneous-readout","text":"Parameter Description index_unconverted Current unconverted index of device index_converted Current converted index of device datetime Current time in device battery_days Battery life in days input_pulse_weight The weight of input pulses parameter_mapping_version The parameter mapping of the device temperature_base Base Temperature temperature_limit_low Low threshold for temperature temperature_limit_high High threshold for temperature temperature Current gas temperature pressure_base Base pressure pressure_limit_low Low threshold for pressure pressure_limit_high High threshold for pressure pressure Current gas pressure pressure_2 Current gas pressure, secondary pressure sensor. Only on devices with secondary sensor flowrate_unconverted Current unconverted flowrate flowrate_converted Current converted flowrate compressibility_formula Code for the formula used in the device. firmware_version Firmware version of the device kernel_version Kernel version of the device. Only available on MID devices","title":"Parameters for instantaneous readout"},{"location":"execution_environments/generic_corus/#parameters-for-database-readout","text":"The main difference in AMR perspective between reading databases from a non-MID devices and a MID-device is that the metering index is available in all databases on a MID devices. It is always better to collect the index and calculate the consumption instead of just collecting consumptions as loosing data will make it hard to interpolate. For reading non-MID devices on a schedule shorter than monthly we recommend that reading the metering is done via the read_parameters call on the schedule you like and complement with a monthly read of the monthly database to get a values that is guaranteed to be on the gas day change. Depending on users queue setup and number of meters there will be a delay in reading instantaneous values of the metering index. On MID-devices just read the databases as they are. If a shorter period than hourly is needed use the interval database and set it to the period you like on the device and schedule readout either quarterly or hourly.","title":"Parameters for database readout"},{"location":"execution_environments/generic_corus/#interval","text":"","title":"Interval"},{"location":"execution_environments/generic_corus/#non-mid-devices","text":"Parameter Description record_duration Duration of the database record status Record status end_date End datetime of record consumption_unconverted_interval Unconverted consumption during period consumption_converted_interval Converted consumption during period consumption_unconverted_interval_under_alarm Unconverted consumption under alarm during period consumption_converted_interval_under_alarm Converted consumption under alarm during period temperature_interval_minimum Minimum temperature during period temperature_interval_maximum Maximum temperature during period temperature_interval_average Average temperature during period pressure_interval_minimum Minimum pressure during period pressure_interval_maximum Maximum pressure during period pressure_interval_average Average pressure during period flowrate_unconverted_interval_minimum Minimum unconverted flowrate during period flowrate_unconverted_interval_maximum Maximum unconverted flowrate during period flowrate_converted_interval_minimum Minimum converted flowrate during period flowrate_converted_interval_maximum Maximum converted flowrate during period flowrate_unconverted_interval_average Average unconverted flowrate during period flowrate_converted_interval_average Average converted flowrate during period start_date Start datetime of record","title":"Non MID devices"},{"location":"execution_environments/generic_corus/#mid-devices","text":"All parameters from non-MID including the following: Parameter Description index_unconverted Metering index unconverted gas volume index_converted Metering index converted gas volume counter_unconverted_under_alarm Counter unconverted under alarm counter_converted_under_alarm Counted converted under alarm pressure_2_interval_minimum Minimum pressure during period, secondary sensor. Only available on devices with a secondary pressure sensor pressure_2_interval_maximum Maximum pressure during period, secondary sensor. Only available on devices with a secondary pressure sensor pressure_2_interval_average Average pressure during period, secondary sensor. Only available on devices with a secondary pressure sensor","title":"MID Devices"},{"location":"execution_environments/generic_corus/#hourly","text":"","title":"Hourly"},{"location":"execution_environments/generic_corus/#non-mid-devices_1","text":"Parameter Description record_duration Duration of the database record status Record status end_date End datetime of record consumption_unconverted_hourly Unconverted consumption during period consumption_converted_hourly Converted consumption during period consumption_unconverted_hourly_under_alarm Unconverted consumption under alarm during period consumption_converted_hourly_under_alarm Converted consumption under alarm during period temperature_hourly_minimum Minimum temperature during period temperature_hourly_maximum Maximum temperature during period temperature_hourly_average Average temperature during period pressure_hourly_minimum Minimum pressure during period pressure_hourly_maximum Maximum pressure during period pressure_hourly_average Average pressure during period flowrate_unconverted_hourly_minimum Minimum unconverted flowrate during period flowrate_unconverted_hourly_maximum Maximum unconverted flowrate during period flowrate_converted_hourly_minimum Minimum converted flowrate during period flowrate_converted_hourly_maximum Maximum converted flowrate during period flowrate_unconverted_hourly_average Average unconverted flowrate during period flowrate_converted_hourly_average Average converted flowrate during period start_date Start datetime of record","title":"Non MID devices"},{"location":"execution_environments/generic_corus/#mid-devices_1","text":"All parameters from non-MID including the following: Parameter Description index_unconverted Metering index unconverted gas volume index_converted Metering index converted gas volume counter_unconverted_under_alarm Counter unconverted under alarm counter_converted_under_alarm Counted converted under alarm pressure_2_hourly_minimum Minimum pressure during period, secondary sensor. Only available on devices with a secondary pressure sensor pressure_2_hourly_maximum Maximum pressure during period, secondary sensor. Only available on devices with a secondary pressure sensor pressure_2_hourly_average Average pressure during period, secondary sensor. Only available on devices with a secondary pressure sensor","title":"MID Devices"},{"location":"execution_environments/generic_corus/#daily","text":"","title":"Daily"},{"location":"execution_environments/generic_corus/#non-mid-devices_2","text":"Parameter Description record_duration Duration of the database record status Record status end_date End datetime of record consumption_unconverted_daily Unconverted consumption during period consumption_converted_daily Converted consumption during period consumption_unconverted_daily_under_alarm Unconverted consumption under alarm during period consumption_converted_daily_under_alarm Converted consumption under alarm during period temperature_daily_minimum Minimum temperature during period temperature_daily_maximum Maximum temperature during period temperature_daily_average Average temperature during period pressure_daily_minimum Minimum pressure during period pressure_daily_maximum Maximum pressure during period pressure_daily_average Average pressure during period flowrate_unconverted_daily_minimum Minimum unconverted flowrate during period flowrate_unconverted_daily_maximum Maximum unconverted flowrate during period flowrate_converted_daily_minimum Minimum converted flowrate during period flowrate_converted_daily_maximum Maximum converted flowrate during period flowrate_unconverted_daily_average Average unconverted flowrate during period flowrate_converted_daily_average Average converted flowrate during period start_date Start datetime of record","title":"Non MID devices"},{"location":"execution_environments/generic_corus/#mid-devices_2","text":"All parameters from non-MID including the following: Parameter Description index_unconverted Metering index unconverted gas volume index_converted Metering index converted gas volume counter_unconverted_under_alarm Counter unconverted under alarm counter_converted_under_alarm Counted converted under alarm pressure_2_daily_minimum Minimum pressure during period, secondary sensor. Only available on devices with a secondary pressure sensor pressure_2_daily_maximum Maximum pressure during period, secondary sensor. Only available on devices with a secondary pressure sensor pressure_2_daily_average Average pressure during period, secondary sensor. Only available on devices with a secondary pressure sensor","title":"MID Devices"},{"location":"execution_environments/generic_corus/#monthly","text":"","title":"Monthly"},{"location":"execution_environments/generic_corus/#non-mid-devices_3","text":"Parameter Description record_duration Duration of the database record status Record status end_date End datetime of record consumption_unconverted_monthly Unconverted consumption during period consumption_converted_monthly Converted consumption during period consumption_unconverted_monthly_under_alarm Unconverted consumption under alarm during period consumption_converted_monthly_under_alarm Converted consumption under alarm during period temperature_monthly_minimum Minimum temperature during period temperature_monthly_maximum Maximum temperature during period temperature_monthly_average Average temperature during period pressure_monthly_minimum Minimum pressure during period pressure_monthly_maximum Maximum pressure during period pressure_monthly_average Average pressure during period flowrate_unconverted_monthly_minimum Minimum unconverted flowrate during period flowrate_unconverted_monthly_maximum Maximum unconverted flowrate during period flowrate_converted_monthly_minimum Minimum converted flowrate during period flowrate_converted_monthly_maximum Maximum converted flowrate during period index_unconverted Metering index unconverted gas volume at end_date of record index_converted Metering index converted gas volume at end_date of record counter_unconverted_under_alarm Counter unconverted under alarm at end_date of record counter_converted_under_alarm Counter converted under alarm at end_date of record consumption_unconverted_monthly_maximum Maximum unconverted consumption of interval period during month consumption_unconverted_monthly_maximum_date Date when maximum unconverted consumption was registered. consumption_converted_monthly_maximum Maximum converted consumption of interval period during month consumption_converted_monthly_maximum_date Date when maximum converted consumption was registered. flowrate_unconverted_monthly_average Average unconverted flowrate during period flowrate_converted_monthly_average Average converted flowrate during period start_date Start datetime of record","title":"Non MID devices"},{"location":"execution_environments/generic_corus/#mid-devices_3","text":"All parameters from non-MID including the following: Parameter Description pressure_2_monthly_minimum Minimum pressure during period, secondary sensor. Only available on devices with a secondary pressure sensor pressure_2_monthly_maximum Maximum pressure during period, secondary sensor. Only available on devices with a secondary pressure sensor pressure_2_monthly_average Average pressure during period, secondary sensor. Only available on devices with a secondary pressure sensor","title":"MID Devices"},{"location":"execution_environments/generic_lis200/","text":"Generic LIS200 # Execution environment name: generic_lis200 Function Calls # Read Single Value: read_single_value Read Archive By Date: read_archive_by_date Read Archive By Offset Seconds: read_archive_by_offset_seconds Set Time: set_time Read Single Value # read_single_value Argument Type Required Description address string yes The LIS-200 address to read # example issuing an on demand amr task via the API { \"meter_device\" : \"890b61bb-6594-48db-ad90-584d56754455\" , \"task_call\" : \"read_single_value\" , \"task_kwargs\" : { \"address\" : ' 2.301 ' , }, \"run_offset_seconds\" : 0 , \"meter_execution_type\" : \"generic_lis200\" } Read archive by date # read_archive_by_date In LIS200 saved data are stored archives and you need to specify which one you need to read. The archives can have different structures and the important thing to know is the control position of the archive, ie. which data entry in an archive row represents the timestamp that you want to use as reference for the read. Archives are read from the oldest data to new newest data, so start_data represents the oldest data in the database Reading by date is useful to re-read old data. Maybe a task failed to collect them or there was a network error or device error. Argument Type Required Description start_dat datetime yes The date for the oldest data to start reading from. end_date datetime no The date to stop reading. If not proved all data from the start date will be read up until the latest entry archive_number int yes The number of the archive you wish to read control_position int yes The position of the timestamp in the archive # example issuing an on demand amr task via the API { \"meter_device\" : \"890b61bb-6594-48db-ad90-584d56754455\" , \"task_call\" : \"read_archive_by_date\" , \"task_kwargs\" : { \"start_date\" : \"2020-01-15T06:00:00+0100\" , \"end_date\" : \"2020-01-16T06:00:00+0100\" , \"archive_number\" : 3 , \"control_position\" : 3 }, \"run_offset_seconds\" : 0 , \"meter_execution_type\" : \"generic_lis200\" } Read archive by offset # read_archive_by_offset_seconds Similar to reading by date but instead you use the current time and read backwards by the offset seconds. For example offset_seconds=900 would read the last 15 min in an archive. Best used with a scheduled AmrTask. Argument Type Required Description offset_seconds int yes Number of seconds to read by. archive_number int yes The number of the archive you wish to read control_position int yes The position of the timestamp in the archive Note When using with a scheduled AmrTask that also is put in an offset schedule you might need to increase the offset to get all requested values. # example issuing an on demand amr task via the API { \"meter_device\" : \"890b61bb-6594-48db-ad90-584d56754455\" , \"task_call\" : \"read_archive_by_offset_seconds\" , \"task_kwargs\" : { \"offset_seconds\" : 1000 , \"archive_number\" : 3 , \"control_position\" : 3 }, \"run_offset_seconds\" : 0 , \"meter_execution_type\" : \"generic_lis200\" } Set time # set_time Will sync the time in the meter with the server. No parameters needed. Argument Type Required Description # example issuing an on demand amr task via the API { \"meter_device\" : \"890b61bb-6594-48db-ad90-584d56754455\" , \"task_call\" : \"set_time\" , \"task_kwargs\" : {}, \"run_offset_seconds\" : 0 , \"meter_execution_type\" : \"generic_lis200\" }","title":"Generic LIS200"},{"location":"execution_environments/generic_lis200/#generic-lis200","text":"Execution environment name: generic_lis200","title":"Generic LIS200"},{"location":"execution_environments/generic_lis200/#function-calls","text":"Read Single Value: read_single_value Read Archive By Date: read_archive_by_date Read Archive By Offset Seconds: read_archive_by_offset_seconds Set Time: set_time","title":"Function Calls"},{"location":"execution_environments/generic_lis200/#read-single-value","text":"read_single_value Argument Type Required Description address string yes The LIS-200 address to read # example issuing an on demand amr task via the API { \"meter_device\" : \"890b61bb-6594-48db-ad90-584d56754455\" , \"task_call\" : \"read_single_value\" , \"task_kwargs\" : { \"address\" : ' 2.301 ' , }, \"run_offset_seconds\" : 0 , \"meter_execution_type\" : \"generic_lis200\" }","title":"Read Single Value"},{"location":"execution_environments/generic_lis200/#read-archive-by-date","text":"read_archive_by_date In LIS200 saved data are stored archives and you need to specify which one you need to read. The archives can have different structures and the important thing to know is the control position of the archive, ie. which data entry in an archive row represents the timestamp that you want to use as reference for the read. Archives are read from the oldest data to new newest data, so start_data represents the oldest data in the database Reading by date is useful to re-read old data. Maybe a task failed to collect them or there was a network error or device error. Argument Type Required Description start_dat datetime yes The date for the oldest data to start reading from. end_date datetime no The date to stop reading. If not proved all data from the start date will be read up until the latest entry archive_number int yes The number of the archive you wish to read control_position int yes The position of the timestamp in the archive # example issuing an on demand amr task via the API { \"meter_device\" : \"890b61bb-6594-48db-ad90-584d56754455\" , \"task_call\" : \"read_archive_by_date\" , \"task_kwargs\" : { \"start_date\" : \"2020-01-15T06:00:00+0100\" , \"end_date\" : \"2020-01-16T06:00:00+0100\" , \"archive_number\" : 3 , \"control_position\" : 3 }, \"run_offset_seconds\" : 0 , \"meter_execution_type\" : \"generic_lis200\" }","title":"Read archive by date"},{"location":"execution_environments/generic_lis200/#read-archive-by-offset","text":"read_archive_by_offset_seconds Similar to reading by date but instead you use the current time and read backwards by the offset seconds. For example offset_seconds=900 would read the last 15 min in an archive. Best used with a scheduled AmrTask. Argument Type Required Description offset_seconds int yes Number of seconds to read by. archive_number int yes The number of the archive you wish to read control_position int yes The position of the timestamp in the archive Note When using with a scheduled AmrTask that also is put in an offset schedule you might need to increase the offset to get all requested values. # example issuing an on demand amr task via the API { \"meter_device\" : \"890b61bb-6594-48db-ad90-584d56754455\" , \"task_call\" : \"read_archive_by_offset_seconds\" , \"task_kwargs\" : { \"offset_seconds\" : 1000 , \"archive_number\" : 3 , \"control_position\" : 3 }, \"run_offset_seconds\" : 0 , \"meter_execution_type\" : \"generic_lis200\" }","title":"Read archive by offset"},{"location":"execution_environments/generic_lis200/#set-time","text":"set_time Will sync the time in the meter with the server. No parameters needed. Argument Type Required Description # example issuing an on demand amr task via the API { \"meter_device\" : \"890b61bb-6594-48db-ad90-584d56754455\" , \"task_call\" : \"set_time\" , \"task_kwargs\" : {}, \"run_offset_seconds\" : 0 , \"meter_execution_type\" : \"generic_lis200\" }","title":"Set time"}]}